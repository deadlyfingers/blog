<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-10-17T12:11:49+01:00</updated><id>/</id><title type="html">Deadlyfingers.net</title><subtitle>Developer blog</subtitle><author><name>David Douglas</name></author><entry><title type="html">Migrating from Wordpress blog to GitHub Pages</title><link href="/info/migrating-from-wordpress-to-github-pages" rel="alternate" type="text/html" title="Migrating from Wordpress blog to GitHub Pages" /><published>2018-10-16T00:00:00+01:00</published><updated>2018-10-16T00:00:00+01:00</updated><id>/info/migrating-from-wordpress-to-github-pages</id><content type="html" xml:base="/info/migrating-from-wordpress-to-github-pages">Wordpress has done a decent job of running my blog. 
But I've longer for something leaner for writing developer code stories as the rich **HTML** text editor tends to introduce too much complexity. 

&lt;!--more--&gt;

Initially I thought about writing a custom blog web app but decided it would be better to consider existing platforms that can handle my requirements.
Ultimately I decided to migrate my Wordpress blog to GitHub pages.

Here's a bunch of things I like with GitHub Pages compared to Wordpress:
- Rather than edit posts in small scroll view I can use a desktop code editor like [VS Code](https://code.visualstudio.com/) which supports full screen editing of text and [live code preview](https://code.visualstudio.com/docs/languages/markdown#_editor-and-preview-synchronization).
- Use GitHub as server - I don't need to provide my own hosting or database.
- Markdown is much easier for writing developer documentation and inline code snippets, but I can also inject HTML when needed.

However there are some downsides during this process:
- Don't expect the migration of Wordpress HTML to MD files to be perfect! In my case the conversion tool had problems preserving spacing in code blocks and some character conversions may need fixed.
- By default all updates will be public on github so you have to think about that if you need to support private posts.

## How to migrate from Wordpress to GitHub Pages (Jekyll)

&gt; GitHub Pages is powered by [Jekyll](https://jekyllrb.com/). To import your Wordpress archive and run Jekyll locally you will need to install [Ruby 2.1](https://www.ruby-lang.org/en/downloads/) or better.

1. If you are migrating an existing blog you have to export your content from the Wordpress admin control panel.  
    `https://YOUR-USER-NAME.wordpress.com/wp-admin/export.php`

2. Import the Wordpress XML archive file as mentioned on the [import wordpress to Jekyll docs](http://import.jekyllrb.com/docs/wordpressdotcom/)
    
    &gt; Install Ruby Gems

    ```shell
    gem install jekyll-import
    gem install hpricot
    gem install open_uri_redirections
    ```
    &gt; Convert Wordpress XML archive to HTML files and download images to 'assets' directory.
    
    ```shell
    ruby -rubygems -e 'require &quot;jekyll-import&quot;;
    JekyllImport::Importers::WordpressDotCom.run({
      &quot;source&quot; =&gt; &quot;C:/Users/USERNAME/Downloads/REPLACE_USING_YOUR_FILE_NAME.wordpress.YYYY-MM-DD.xml&quot;,
      &quot;no_fetch_images&quot; =&gt; false,
      &quot;assets_folder&quot; =&gt; &quot;assets/images&quot;
    })'
    ```

3. Convert HTML files to Markdown files. You can try any number of tools to see what works best for you. I tried various ones including the [reverse_markdown](https://github.com/xijo/reverse_markdown) Ruby gem and the [html2text](https://github.com/aaronsw/html2text.git) Python script. To help batch process the files I created a [Wordpress HTML to MD gist](https://gist.github.com/deadlyfingers/2023c61cbac83bb613393f262693cdf4) to find any **\*.html** files in the '_posts' directory and convert them all to **\*.md** files using [reverse_markdown](https://rubygems.org/gems/reverse-markdown) gem.
    
    &gt; **'wordpress-html-to-md.rb'** gist usage:

    ```shell
    gem install reverse_markdown
    ```
    ```shell
    ruby ./wordpress-html-to-md.rb &quot;_posts&quot;
    ```
    &lt;script src=&quot;https://gist.github.com/deadlyfingers/2023c61cbac83bb613393f262693cdf4.js&quot;&gt;&lt;/script&gt;

    &gt; **html2text** usage:

    ```shell
    ./html2text.py C:/Users/USERNAME/git/blog/_posts/YYYY-MM-DD-filename.html
    ```
    &gt; NB. Don't use &quot;\\&quot; in path otherwise you will get file not found error, use &quot;/&quot; in path instead.

4. To show code syntax highlights you will need to add some styles for [Rouge](https://github.com/jneen/rouge) (GitHub Page's syntax highlighter). You can use Rougify to copy GitHub's code syntax highlighting to a stylesheet.
    ```shell
    gem install rouge
    ```
    ```shell
    rougify style github &gt; _sass/styles/_rouge.scss
    ```

After this you might decide to apply one of the [built-in GitHub Pages themes](https://pages.github.com/themes/) or use a [remote theme](https://github.com/benbalter/jekyll-remote-theme) or [create your own theme](https://jekyllrb.com/docs/themes/). In my case I added the [Foundation XY-Grid](https://foundation.zurb.com/sites/docs/xy-grid.html) module for responsive design grid layouts. I have also included a list of references below which I found useful during the creation of this new blog.

## References

### GitHub Pages settings
- [GitHub Pages](https://pages.github.com/)
- [Using Jekyll](https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/)
- [Themes](https://pages.github.com/themes/)
- [Default Plugins](https://help.github.com/articles/configuring-jekyll-plugins/#default-plugins)
- [markdown: kramdown](https://help.github.com/articles/updating-your-markdown-processor-to-kramdown/)
- [highlighter: rouge](https://help.github.com/articles/using-syntax-highlighting-on-github-pages/)

### Jekyll blog
- [Jekyll structure](https://jekyllrb.com/docs/structure/)
- [Jekyll cheatsheet](https://devhints.io/jekyll)
- [Jekyll md cheatsheet](https://github.com/rstacruz/cheatsheets/blob/master/jekyll.md)
- [Posts](https://jekyllrb.com/docs/posts/)
- [Assets](https://jekyllrb.com/docs/assets/)
- [Sass/SCSS](https://jekyllrb.com/docs/assets/#sassscss)
- [Generate GitHub Rouge SASS](http://ben.balter.com/jekyll-style-guide/themes/)
- [Linking to pages](https://jekyllrb.com/docs/liquid/tags/#link)

### YML Config reference docs
- [YML Syntax](https://help.github.com/articles/page-build-failed-config-file-error/#troubleshooting-_configyml-syntax-errors)
- [Jekyll Configuration](https://jekyllrb.com/docs/configuration/default/)
- [GitHub Pages setup](https://jekyllrb.com/docs/github-pages/)
- [GitHub Pages dependencies](https://pages.github.com/versions/)
- [List of TimeZones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)

### Disqus
- [Adding Disqus to Jekyll blog](http://sgeos.github.io/jekyll/disqus/2016/02/15/adding-disqus-to-a-jekyll-blog.html)

### Foundation
- [XY-Grid](https://foundation.zurb.com/sites/docs/xy-grid.html)</content><author><name>David Douglas</name></author><category term="GitHub Pages" /><category term="Jekyll" /><category term="Ruby" /><category term="Wordpress" /><category term="Blog" /><category term="Markdown" /><category term="Website" /><summary type="html">Wordpress has done a decent job of running my blog. But I’ve longer for something leaner for writing developer code stories as the rich HTML text editor tends to introduce too much complexity.</summary></entry><entry><title type="html">Custom Vuforia VuMarks to identify and monitor IoT Devices with HoloLens</title><link href="/code/custom-vuforia-vumarks-to-identify-iot-devices-with-hololens" rel="alternate" type="text/html" title="Custom Vuforia VuMarks to identify and monitor IoT Devices with HoloLens" /><published>2018-09-24T15:47:27+01:00</published><updated>2018-09-24T15:47:27+01:00</updated><id>/code/custom-vuforia-vumarks-to-identify-iot-devices-with-hololens</id><content type="html" xml:base="/code/custom-vuforia-vumarks-to-identify-iot-devices-with-hololens">Some of the most popular experiences of Augmented or Mixed Reality are currently in a gaming or an immersive 3D form. But what makes headsets like the HoloLens so different and special is the ability to still see into the physical world. It is this link that opens up so many new opportunities and future possibilities of spatial computing. An interesting scenario is using HoloLens to interact with IoT devices in the real world. Just by gazing at devices around a room imagine you could identify a specific IoT device to review its real-time telemetry and control it over the air!

## Setting up some sample IoT devices to interact with...

To test out this scenario the first thing we need here is access to some IoT devices to identify. In our case and for sake of simplicity we use some simulated IoT devices using the [Azure IoT Solution Accelerators](https://www.azureiotsolutions.com/Accelerators) **Remote Monitoring** sample which you can try out.

 ![]({{ site.baseurl }}/assets/images/Azure-IoT-000-Solution-Accelerators.png)

 ![]({{ site.baseurl }}/assets/images/Azure-IoT-002-launch-.png)

Once the Azure IoT Remote Monitoring solution is provisioned and ready you can select it to review a list of devices. This provides us with the list of Device Name Ids we will use to generate the VuForia VuMarks in the next steps.

 ![]({{ site.baseurl }}/assets/images/Azure-IoT-003-devices.png)

## Identifying IoT devices in the real-world using VuForia VuMarks

The second thing we need is to be able to identify each IoT device in HoloLens. One approach we tried during a HoloLens hack was to use [Vuforia VuMarks](https://library.vuforia.com/features/objects/vumark.html) to identify each device. A VuMark template contains a particular type of encoded data; numeric, string or raw bytes. Initially I tried out the [default numeric type VuMarks](https://library.vuforia.com/content/dam/vuforia-library/docs/target_samples/unity/mars_vumarks.pdf) from the [VuForia Samples](https://library.vuforia.com/content/vuforia-library/en/articles/Solution/sample-apps-target-pdfs.html) to check everything was working before trying anything more complex. Bear in mind there will also be a number of physical and environmental factors including VuMark placement, size and lighting conditions in the area to test and consider.  
**Tip:** I found it useful to test the VuMarks by saving all the generated images on my iPhone and testing them in the Unity Editor using the built-in web cam.

### Creating a custom VuMark

I used the [VuForia VuMark Illustrator template](https://library.vuforia.com/articles/Solution/Designing-a-VuMark-in-Adobe-Illustrator.html) to create a custom VuMark. In my case I wanted to a support a **32 character length string** to contain a GUID so I created a string type VuMark with **280 data elements**. To save time designing your own VuMark you can download my finished [custom GUID VuMark SVG]({{ site.baseurl }}/assets/media/guidVumark.svg). If you want to create your own VuMark I've included a list of VuMark element requirements below so you can get an idea of how complex the design would need to be and compare how many elements are required for each data type:

| Id length | String elements required | Byte elements required |
| --- | --- | --- |
| 1 | 35 | 40 |
| 4 | 56 | 64 |
| 8 | 84 | 96 |
| 10 | 98 | 112 |
| 11 | 112 | 120 |
| 12 | 119 | 128 |
| 14 | 133 | 144 |
| 16 | 147 | 160 |
| 18 | 161 | 176 |
| 20 | 182 | 208 |
| 22 | 196 | 224 |
| 24 | 210 | 240 |
| 32 | **280** | 320 |
| 48 | 406 | 464 |
| 64 | 546 | 624 |
| 100 | 840 | 928 |

| Maximum numeric Id | Numeric elements required |
| --- | --- |
| 9 | 28 |
| 99 | 31 |
| 999 | 34 |
| 9999 | 38 |
| 9 x5 | 41 |
| 9 x6 | 50 |
| 9 x7 | 54 |
| 9 x8 | 57 |
| 9 x9 | 60 |
| 9 x10 | 64 |
| 9 x11 | 67 |
| 9 x12 | 70 |
| 9 x13 | 74 |
| 9 x14 | 77 |
| 9 x15 | 80 |
| 9 x16 | 84 |
| 9 x17 | 87 |
| 9 x18 | 90 |
| 9 x19 | 94 |

For more info on designing VuMarks you can [download the VuMark design guide](https://developer.vuforia.com/sites/default/files/VuMark%20Design%20Guide.pdf) or [view design guide docs](https://library.vuforia.com/articles/Training/VuMark-Design-Guide). I also found the [video explaining the VuMark design process](https://www.youtube.com/watch?v=YXMiDRyvqzk) to be most helpful. NB: To design your own custom VuMarks you will need [Adobe Illustrator](https://www.adobe.com/uk/products/illustrator.html) to run the [VuMark template scripts](https://developer.vuforia.com/vui/auth/login?url=/downloads/tool%3Fd%3Dwindows-4851025-17-4124%26retU).

**Illustrator / VuMark Scripts troubleshooting notes:**

- You may have to restart Illustrator after copying the scripts into the `C:\Program Files\Adobe\Adobe Illustrator CC 2018\Presets\en_US\Scripts` directory.
- If you hit an error when setting up a new VuMark using the [Illustrator scripts v6.0.112](https://developer.vuforia.com/vui/auth/login?url=/downloads/tool%3Fd%3Dwindows-4851025-17-4124%26retU) then check you have Adobe's [Myriad Pro fonts installed](https://developer.vuforia.com/forum/vumark-design-tools/errors-illustrator-scripts#comment-55448).
- If you can't see the Illustrator canvas or the document area is blank or black then you might have to disable GPU acceleration under Preferences \&gt; Performance.

### Creating custom VuMark database for Unity

Once you've designed your custom VuMark in Illustrator and it passes all the tests you will be ready to export your VuMark Template artwork. If you don't have your own design ready you can [download my GUID VuMark SVG artwork]({{ site.baseurl }}/assets/media/guidVumark.svg).

 ![]({{ site.baseurl }}/assets/images/VuMark-Creation-Illustrator-280-elements-32-string.png)

**Note:** If you're starting a new design it's preferable to avoid rotational symmetry in your VuMark's border or contour otherwise you will have some additional work to do, as well as this the validation scripts don't seem to provide a clear indication if this is completed correctly. You might also notice the Border and Clear Space width is only shown as &quot;VERIFY&quot; status - this check is left to the designer to manually check that the magenta overlay around the VuMark contour falls within the border and clear space boundary.

1. If you haven't used VuForia before you will have to create a developer account and get a free license key for development in Unity.
 ![]({{ site.baseurl }}/assets/images/VuForia-Developer-License-Key.png)
2. Create a new VuMarks database.
 ![]({{ site.baseurl }}/assets/images/VuForia-Database.png)
3. Upload the [custom VuMark SVG artwork file]({{ site.baseurl }}/assets/media/guidVumark.svg) into your VuMark database. **Note:** You should set the width of the VuMark in relation to Unity's unit of measurement which is in **meters**. In my case I want to recognize the VuMark on my iPhone which is 6 cm wide therefore I use a value of &quot; **0.06**&quot; m.
 ![]({{ site.baseurl }}/assets/images/VuMark-Upload-Export-SVG.png)
4. Select your VuMark template target to **download** as your VuMark database.
 ![]({{ site.baseurl }}/assets/images/VuForia-VuMark-Database.png)
5. Download database for **Unity Editor**.
 ![]({{ site.baseurl }}/assets/images/VuForia-Unity-Database.png)
6. **Import** your VuMarks database package into Unity project. If you don't have your own Unity project you can setup the [Mixed Reality IoT Monitoring sample](https://github.com/deadlyfingers/MixedRealityIoTMonitoring) to get started.
 ![]({{ site.baseurl }}/assets/images/Unity-Import-VuMarks-Package.png)
7. In Unity scene check the VuMark Behavior is setup correctly with your custom **VuMark Database and Template** and has **Extended Tracking** enabled for Mixed Reality.
 ![]({{ site.baseurl }}/assets/images/Unity-VuMark-Behaviour.png)
8. Open the VuForia AR Camera configuration settings to enter your **VuForia developer license key** and to **load and activate the VuMark database**.
 ![]({{ site.baseurl }}/assets/images/VuForia-ARCamera-Config.jpg)
9. Generate the VuMark images for each Device Id you want to recognize.
 ![]({{ site.baseurl }}/assets/images/VuForia-Generate-VuMark-GUID.png)

For my sample IoT devices I generated the following device Ids; &quot;chiller-01.0&quot;, &quot;chiller-02.0&quot;, &quot;elevator-01.0&quot;, &quot;elevator-02.0&quot;, &quot;furnace-01.0&quot;.

&lt;div class=&quot;grid-x&quot;&gt;
  &lt;div class=&quot;small-6 cell&quot;&gt;
    &lt;img src=&quot;{{ site.baseurl }}/assets/images/guidVumark_chiller-01.0.png&quot; alt=&quot;chiller-01.0&quot;/&gt;
  &lt;/div&gt;
  &lt;div class=&quot;small-6 cell&quot;&gt;
    &lt;img src=&quot;{{ site.baseurl }}/assets/images/guidVumark_chiller-02.0.png&quot; alt=&quot;chiller-02.0&quot;/&gt;
  &lt;/div&gt;
  &lt;div class=&quot;small-6 cell&quot;&gt;
    &lt;img src=&quot;{{ site.baseurl }}/assets/images/guidVumark_elevator-01.0.png&quot; alt=&quot;elevator-01.0&quot;/&gt;
  &lt;/div&gt;
  &lt;div class=&quot;small-6 cell&quot;&gt;
    &lt;img src=&quot;{{ site.baseurl }}/assets/images/guidVumark_elevator-02.0.png&quot; alt=&quot;elevator-02.0&quot;/&gt;
  &lt;/div&gt;
  &lt;div class=&quot;small-6 cell&quot;&gt;
    &lt;img src=&quot;{{ site.baseurl }}/assets/images/guidVumark_furnace-01.0.png&quot; alt=&quot;furnace-01.0&quot;/&gt;
  &lt;/div&gt;
&lt;/div&gt;

**Tip:** Save the generated VuMark images to iPhone / Android to test with. (I just saved the generated VuMark PNG images to my [OneDrive](https://onedrive.live.com/) images folder to sync onto my iPhone.)

## Running the sample Mixed Reality IoT Monitoring Unity project

To run the [Mixed Reality IoT Monitoring Unity project](https://github.com/deadlyfingers/MixedRealityIoTMonitoring) you will also need to setup the Azure Function APIs to get the device data from the Azure IoT Remote Monitoring sample.

### Azure IoT Device Functions (Nodejs backend)

1. Fork the [Azure IoT Device Functions project](https://github.com/deadlyfingers/IoTDeviceFunctions) on github.
2. Sign in to your [Azure portal](https://portal.azure.com)
3. Create a new Azure Function. NB: Ensure your **Function app settings** is using **version 2**
4. Add the following environment variables using your **Azure Function app settings** :
  - IOTHUB\_CONNECTION\_STRING
  - TSI\_FQDN
  - AD\_APP\_ID
  - AD\_APP\_KEY
  - AD\_TENANT\_DOMAIN NAME &quot;\*.onmicrosoft.com&quot;
5. To deploy your Function app select **Platform Features \&gt; Deployment Options \&gt; Setup \&gt; GitHub** and choose your forked repo.

## Next steps...

Using VuForia VuMarks we are able to identify an IoT device using a HoloLens. Then using the recognized device Id as a param we can poll an Azure Functions endpoint to return the device's telemetry. The next steps in this scenario would be to add buttons to call methods listed in the device's payload.</content><author><name>David Douglas</name></author><category term="IoT" /><category term="HoloLens" /><category term="Mixed Reality" /><category term="VuForia" /><category term="VuMark" /><category term="Data Visualisation" /><summary type="html">Some of the most popular experiences of Augmented or Mixed Reality are currently in a gaming or an immersive 3D form. But what makes headsets like the HoloLens so different and special is the ability to still see into the physical world. It is this link that opens up so many new opportunities and future possibilities of spatial computing. An interesting scenario is using HoloLens to interact with IoT devices in the real world. Just by gazing at devices around a room imagine you could identify a specific IoT device to review its real-time telemetry and control it over the air! Setting up some sample IoT devices to interact with… To test out this scenario the first thing we need here is access to some IoT devices to identify. In our case and for sake of simplicity we use some simulated IoT devices using the Azure IoT Solution Accelerators Remote Monitoring sample which you can try out. Once the Azure IoT Remote Monitoring solution is provisioned and ready you can select it to review a list of devices. This provides us with the list of Device Name Ids we will use to generate the VuForia VuMarks in the next steps. Identifying IoT devices in the real-world using VuForia VuMarks The second thing we need is to be able to identify each IoT device in HoloLens. One approach we tried during a HoloLens hack was to use Vuforia VuMarks to identify each device. A VuMark template contains a particular type of encoded data; numeric, string or raw bytes. Initially I tried out the default numeric type VuMarks from the VuForia Samples to check everything was working before trying anything more complex. Bear in mind there will also be a number of physical and environmental factors including VuMark placement, size and lighting conditions in the area to test and consider. Tip: I found it useful to test the VuMarks by saving all the generated images on my iPhone and testing them in the Unity Editor using the built-in web cam. Creating a custom VuMark I used the VuForia VuMark Illustrator template to create a custom VuMark. In my case I wanted to a support a 32 character length string to contain a GUID so I created a string type VuMark with 280 data elements. To save time designing your own VuMark you can download my finished custom GUID VuMark SVG. If you want to create your own VuMark I’ve included a list of VuMark element requirements below so you can get an idea of how complex the design would need to be and compare how many elements are required for each data type: Id length String elements required Byte elements required 1 35 40 4 56 64 8 84 96 10 98 112 11 112 120 12 119 128 14 133 144 16 147 160 18 161 176 20 182 208 22 196 224 24 210 240 32 280 320 48 406 464 64 546 624 100 840 928 Maximum numeric Id Numeric elements required 9 28 99 31 999 34 9999 38 9 x5 41 9 x6 50 9 x7 54 9 x8 57 9 x9 60 9 x10 64 9 x11 67 9 x12 70 9 x13 74 9 x14 77 9 x15 80 9 x16 84 9 x17 87 9 x18 90 9 x19 94 For more info on designing VuMarks you can download the VuMark design guide or view design guide docs. I also found the video explaining the VuMark design process to be most helpful. NB: To design your own custom VuMarks you will need Adobe Illustrator to run the VuMark template scripts. Illustrator / VuMark Scripts troubleshooting notes: You may have to restart Illustrator after copying the scripts into the C:\Program Files\Adobe\Adobe Illustrator CC 2018\Presets\en_US\Scripts directory. If you hit an error when setting up a new VuMark using the Illustrator scripts v6.0.112 then check you have Adobe’s Myriad Pro fonts installed. If you can’t see the Illustrator canvas or the document area is blank or black then you might have to disable GPU acceleration under Preferences &amp;gt; Performance. Creating custom VuMark database for Unity Once you’ve designed your custom VuMark in Illustrator and it passes all the tests you will be ready to export your VuMark Template artwork. If you don’t have your own design ready you can download my GUID VuMark SVG artwork. Note: If you’re starting a new design it’s preferable to avoid rotational symmetry in your VuMark’s border or contour otherwise you will have some additional work to do, as well as this the validation scripts don’t seem to provide a clear indication if this is completed correctly. You might also notice the Border and Clear Space width is only shown as “VERIFY” status - this check is left to the designer to manually check that the magenta overlay around the VuMark contour falls within the border and clear space boundary. If you haven’t used VuForia before you will have to create a developer account and get a free license key for development in Unity. Create a new VuMarks database. Upload the custom VuMark SVG artwork file into your VuMark database. Note: You should set the width of the VuMark in relation to Unity’s unit of measurement which is in meters. In my case I want to recognize the VuMark on my iPhone which is 6 cm wide therefore I use a value of “ 0.06” m. Select your VuMark template target to download as your VuMark database. Download database for Unity Editor. Import your VuMarks database package into Unity project. If you don’t have your own Unity project you can setup the Mixed Reality IoT Monitoring sample to get started. In Unity scene check the VuMark Behavior is setup correctly with your custom VuMark Database and Template and has Extended Tracking enabled for Mixed Reality. Open the VuForia AR Camera configuration settings to enter your VuForia developer license key and to load and activate the VuMark database. Generate the VuMark images for each Device Id you want to recognize. For my sample IoT devices I generated the following device Ids; “chiller-01.0”, “chiller-02.0”, “elevator-01.0”, “elevator-02.0”, “furnace-01.0”. Tip: Save the generated VuMark images to iPhone / Android to test with. (I just saved the generated VuMark PNG images to my OneDrive images folder to sync onto my iPhone.) Running the sample Mixed Reality IoT Monitoring Unity project To run the Mixed Reality IoT Monitoring Unity project you will also need to setup the Azure Function APIs to get the device data from the Azure IoT Remote Monitoring sample. Azure IoT Device Functions (Nodejs backend) Fork the Azure IoT Device Functions project on github. Sign in to your Azure portal Create a new Azure Function. NB: Ensure your Function app settings is using version 2 Add the following environment variables using your Azure Function app settings : IOTHUB_CONNECTION_STRING TSI_FQDN AD_APP_ID AD_APP_KEY AD_TENANT_DOMAIN NAME “*.onmicrosoft.com” To deploy your Function app select Platform Features &amp;gt; Deployment Options &amp;gt; Setup &amp;gt; GitHub and choose your forked repo. Next steps… Using VuForia VuMarks we are able to identify an IoT device using a HoloLens. Then using the recognized device Id as a param we can poll an Azure Functions endpoint to return the device’s telemetry. The next steps in this scenario would be to add buttons to call methods listed in the device’s payload.</summary></entry><entry><title type="html">Analytics for Mixed Reality</title><link href="/code/analytics-for-mixed-reality" rel="alternate" type="text/html" title="Analytics for Mixed Reality" /><published>2018-09-17T16:36:26+01:00</published><updated>2018-09-17T16:36:26+01:00</updated><id>/code/analytics-for-mixed-reality</id><content type="html" xml:base="/code/analytics-for-mixed-reality">## Behind every good user experience is great analytics

If you ever designed or developed client side applications or websites you've probably integrated with an analytics service to provide telemetry data to help make informed design choices and development decisions to improve user experience and business outcomes.  
One of the key performance indicators is when you track steps or funnel operations as conversions to calculate a conversion rate for each session. You would want to know how the conversion rate can be improved upon and a good idea would be to watch out for the steps with a high bounce rate where users are dropping off. If the bounce rate is very high then there might even be a blocker or flaw in regards to the user. Either way we can understand the benefit for the collection and study of analytics which is essential for providing the insights that will help designers and developers craft better user experiences and advance product development.

# Application Insights for Unity

If you're a Unity developer or you develop in VR, AR, MR or XR you might have stuck the issue of gathering analytics onto the backlog but it should be one of the first items done so you can use it to help plan and prioritize the other features. To help you get started I've made an [Application Insights for Unity sample](https://github.com/Unity3dAzure/UnityApplicationInsights) so you can start logging telemetry in just a few minutes! All you have to do to add this to your existing Unity app or game is drop the **Unity Application Insights** script onto a Game Object, add your Application Insights **Instrumentation key** and you will be all set to record valuable user session telemetry automatically. After that all you have to do it wait around 5 mins for the telemetry to display in the **Application Insights Usage** section in Azure. You can also extend this in you own app or game to record any custom events or metrics you want to know about. But right out of the box (without any additional effort on your part) you will be able to visualize telemetry for users, sessions and user flow and their journey across the scenes of your Unity app or game. Here are just some of the visualizations already built-in to Application Insights in the Azure portal:

### User Flows

Chart user flow across Unity scene changes and split by custom or interaction events.

 ![Unity App Insights User Flow]({{ site.baseurl }}/assets/images/ApplicationInsights-UserFlows.png)

### Sessions

View users and events during sessions.

 ![Unity App Insights User sessions]({{ site.baseurl }}/assets/images/ApplicationInsights-Sessions.png)

### Funnels

Create funnels by creating step by step conditions to get conversion rates.

 ![Unity App Insights Conversions]({{ site.baseurl }}/assets/images/ApplicationInsights-Funnels.png)

### Retention

Review returning users over a period of time.

 ![Unity App Insights Retention]({{ site.baseurl }}/assets/images/ApplicationInsights-Retention.png)

## Analytics for Mixed Reality interactions

In the Unity project there is also a MR sample to show how to setup Application Insights for recording custom interaction events and metrics in a scene. To use the sample please fork or clone the [Unity Application Insights sample project](https://github.com/Unity3dAzure/UnityApplicationInsights), import the [plugins](https://github.com/Microsoft/HolographicAcademy/raw/Azure-MixedReality-Labs/Azure%20Mixed%20Reality%20Labs/MR%20and%20Azure%20309%20-%20Application%20insights/AppInsights_LabPlugins.unitypackage) and setup **Application Insights** in [Azure portal](https://portal.azure.com) if you haven't already. Once you've got that you can get up and running on HoloLens straight from the Unity Editor:

1. To view the Mixed Reality sample in HoloLens open the scene named &quot;Scene-MR&quot;. (Make sure you've pasted in your Instrumentation key into the Application Insights game object script)
2. Connect to remote HoloLens device using **Window \&gt; XR \&gt; Holographic Emulation** window. Note: Requires the [Holographic Remoting Player](https://www.microsoft.com/en-us/p/holographic-remoting-player/9nblggh4sv40?activetab=pivot%3aoverviewtab) installed and open on HoloLens to get the _Remote Machine_ IP address.
3. Hit **Play** and you will start recording interaction telemetry with the holograms.

The Application Insights MR scripts will record taps, gaze time and object proximity - when users physically &quot;visit&quot; a hologram by moving closer to it.  
You can also create your own custom dashboard templates using [Ibex Dashboard](https://github.com/Azure/ibex-dashboard) (which is another project I helped with) and is designed for visualizing data from Application Insights using [Kusto queries](https://aka.ms/kusto).

 ![]({{ site.baseurl }}/assets/images/IbexDashboard-UnityMR.png)

You can add the [dashboard template for MR](https://gist.github.com/deadlyfingers/e664aaccb748be2f332f462615f6a090) shown above to visualize the telemetry for MR custom events and metrics. Check out the [readme on github](https://github.com/Unity3dAzure/UnityApplicationInsights#custom-visualization-of-unity-ui-and-mr-telemetry) for more info about installing custom Ibex Dashboard templates.</content><author><name>David Douglas</name></author><category term="Application Insights" /><category term="Ibex Dashboard" /><category term="Data Visualisation" /><category term="Azure" /><category term="Unity3D" /><category term="Mixed Reality" /><category term="HoloLens" /><summary type="html">Behind every good user experience is great analytics If you ever designed or developed client side applications or websites you’ve probably integrated with an analytics service to provide telemetry data to help make informed design choices and development decisions to improve user experience and business outcomes. One of the key performance indicators is when you track steps or funnel operations as conversions to calculate a conversion rate for each session. You would want to know how the conversion rate can be improved upon and a good idea would be to watch out for the steps with a high bounce rate where users are dropping off. If the bounce rate is very high then there might even be a blocker or flaw in regards to the user. Either way we can understand the benefit for the collection and study of analytics which is essential for providing the insights that will help designers and developers craft better user experiences and advance product development. Application Insights for Unity If you’re a Unity developer or you develop in VR, AR, MR or XR you might have stuck the issue of gathering analytics onto the backlog but it should be one of the first items done so you can use it to help plan and prioritize the other features. To help you get started I’ve made an Application Insights for Unity sample so you can start logging telemetry in just a few minutes! All you have to do to add this to your existing Unity app or game is drop the Unity Application Insights script onto a Game Object, add your Application Insights Instrumentation key and you will be all set to record valuable user session telemetry automatically. After that all you have to do it wait around 5 mins for the telemetry to display in the Application Insights Usage section in Azure. You can also extend this in you own app or game to record any custom events or metrics you want to know about. But right out of the box (without any additional effort on your part) you will be able to visualize telemetry for users, sessions and user flow and their journey across the scenes of your Unity app or game. Here are just some of the visualizations already built-in to Application Insights in the Azure portal: User Flows Chart user flow across Unity scene changes and split by custom or interaction events. Sessions View users and events during sessions. Funnels Create funnels by creating step by step conditions to get conversion rates. Retention Review returning users over a period of time. Analytics for Mixed Reality interactions In the Unity project there is also a MR sample to show how to setup Application Insights for recording custom interaction events and metrics in a scene. To use the sample please fork or clone the Unity Application Insights sample project, import the plugins and setup Application Insights in Azure portal if you haven’t already. Once you’ve got that you can get up and running on HoloLens straight from the Unity Editor: To view the Mixed Reality sample in HoloLens open the scene named “Scene-MR”. (Make sure you’ve pasted in your Instrumentation key into the Application Insights game object script) Connect to remote HoloLens device using Window &amp;gt; XR &amp;gt; Holographic Emulation window. Note: Requires the Holographic Remoting Player installed and open on HoloLens to get the Remote Machine IP address. Hit Play and you will start recording interaction telemetry with the holograms. The Application Insights MR scripts will record taps, gaze time and object proximity - when users physically “visit” a hologram by moving closer to it. You can also create your own custom dashboard templates using Ibex Dashboard (which is another project I helped with) and is designed for visualizing data from Application Insights using Kusto queries. You can add the dashboard template for MR shown above to visualize the telemetry for MR custom events and metrics. Check out the readme on github for more info about installing custom Ibex Dashboard templates.</summary></entry><entry><title type="html">Unity Web Sockets for Mixed Reality</title><link href="/code/unity-web-sockets-mixed-reality-uwp" rel="alternate" type="text/html" title="Unity Web Sockets for Mixed Reality" /><published>2018-09-16T13:17:16+01:00</published><updated>2018-09-16T13:17:16+01:00</updated><id>/code/unity-web-sockets-mixed-reality-uwp</id><content type="html" xml:base="/code/unity-web-sockets-mixed-reality-uwp">Certain cloud services may offer a Web Socket streaming connection as an alternative to firing repeated REST requests or polling. To make working with REST APIs in Unity more convenient I built a [REST client for Unity](https://github.com/Unity3dAzure/RESTClient) based on [UnityWebRequest](https://docs.unity3d.com/ScriptReference/Networking.UnityWebRequest.html) which supports abstract types for JSON / XML serialisation. But given a real-time scenario like &quot;speech to text&quot; using a Web Socket client instead of REST gives an option to stream the audio data and get intermediate results back which provides responsive feedback to users for an improved user experience.

###### Using Bing Speech API as an example we can see some limitations of using REST API versus the Web Socket protocol:

| Bing Speech | REST | Web Socket |
| --- | --- | --- |
| Audio stream duration | 15 secs | 180 secs - 10 mins |
| Stream audio with intermediate results | No | Yes |

Also when it comes to client app development in Unity there are a couple of very useful message events you receive from the WebSockets server:  
- **End-of-speech detection** so you stop recording on client device.  
- **Phrase detection** so you can pass phrases to [natural language understanding services (LUIS)](https://www.luis.ai/) model.

To use Web Sockets in Unity you can use the [WebSocket-Sharp library](https://github.com/sta/websocket-sharp), but this only supports the Unity Editor and the mono target platforms. In order to use Web Sockets when targeting Windows Mixed Reality headsets you have to use Universal Windows Platform (UWP) APIs like [MessageWebSocket](https://docs.microsoft.com/en-us/uwp/api/windows.networking.sockets.messagewebsocket). To make things easier I have created a common [Unity Web Socket](https://github.com/Unity3dAzure/UnityWebSocket) interface to use WebSocket-Sharp inside the Editor and mono platforms and then use `MessageWebSocket` when targeting the Windows Store App platform for MR headsets.

### Unity Web Socket interface

| API | Description |
| --- | --- |
| ConfigureWebSocket(url) | Configures web socket with url and optional headers |
| ConnectAsync() | Connect to web socket |
| CloseAsync() | Close web socket connection |
| SendAsync(data) | Send binary byte[] or UTF-8 text string with optional callback |
| IsOpen() | Check if web socket status is open |
| Url() | Return the URL being used by the web socket |

### Interface events

```csharp
OnError(object sender, WebSocketErrorEventArgs e);
OnOpen(object sender, EventArgs e);
OnMessage(object sender, WebSocketMessageEventArgs e);
OnClose(object sender, WebSocketCloseEventArgs e);
```

If you are interested in the example above there I have also prepared a [Unity Web Socket demo project](https://github.com/Unity3dAzure/UnityWebSocketDemo) to show Bing Speech service to LUIS for controlling scene game objects using natural speech commands for Mixed Reality scenarios.</content><author><name>David Douglas</name></author><category term="Unity3D" /><category term="UWP" /><category term="Web Sockets" /><category term="Mixed Reality" /><category term="HoloLens" /><summary type="html">Certain cloud services may offer a Web Socket streaming connection as an alternative to firing repeated REST requests or polling. To make working with REST APIs in Unity more convenient I built a REST client for Unity based on UnityWebRequest which supports abstract types for JSON / XML serialisation. But given a real-time scenario like “speech to text” using a Web Socket client instead of REST gives an option to stream the audio data and get intermediate results back which provides responsive feedback to users for an improved user experience. Using Bing Speech API as an example we can see some limitations of using REST API versus the Web Socket protocol: Bing Speech REST Web Socket Audio stream duration 15 secs 180 secs - 10 mins Stream audio with intermediate results No Yes Also when it comes to client app development in Unity there are a couple of very useful message events you receive from the WebSockets server: End-of-speech detection so you stop recording on client device. Phrase detection so you can pass phrases to natural language understanding services (LUIS) model. To use Web Sockets in Unity you can use the WebSocket-Sharp library, but this only supports the Unity Editor and the mono target platforms. In order to use Web Sockets when targeting Windows Mixed Reality headsets you have to use Universal Windows Platform (UWP) APIs like MessageWebSocket. To make things easier I have created a common Unity Web Socket interface to use WebSocket-Sharp inside the Editor and mono platforms and then use MessageWebSocket when targeting the Windows Store App platform for MR headsets. Unity Web Socket interface API Description ConfigureWebSocket(url) Configures web socket with url and optional headers ConnectAsync() Connect to web socket CloseAsync() Close web socket connection SendAsync(data) Send binary byte[] or UTF-8 text string with optional callback IsOpen() Check if web socket status is open Url() Return the URL being used by the web socket Interface events OnError(object sender, WebSocketErrorEventArgs e); OnOpen(object sender, EventArgs e); OnMessage(object sender, WebSocketMessageEventArgs e); OnClose(object sender, WebSocketCloseEventArgs e); If you are interested in the example above there I have also prepared a Unity Web Socket demo project to show Bing Speech service to LUIS for controlling scene game objects using natural speech commands for Mixed Reality scenarios.</summary></entry><entry><title type="html">Querying Application Insights for data visualisation</title><link href="/code/querying-application-insights-for-data-visualisation" rel="alternate" type="text/html" title="Querying Application Insights for data visualisation" /><published>2017-08-30T14:48:00+01:00</published><updated>2017-08-30T14:48:00+01:00</updated><id>/code/querying-application-insights-for-data-visualisation</id><content type="html" xml:base="/code/querying-application-insights-for-data-visualisation">[Ibex dashboard](https://github.com/CatalystCode/ibex-dashboard) is an open source [web app](https://azure.microsoft.com/en-gb/services/app-service/web/) for displaying telemetry data from [Application Insights](https://azure.microsoft.com/en-gb/services/application-insights/). It comes with a number of [sample templates](https://github.com/CatalystCode/ibex-dashboard/tree/master/server/dashboards/preconfigured) including analytics dashboards for [Bots](https://dev.botframework.com/). If you're developing a bot and you want to see how your bot is performing over time then you can select the [Bot Instrumentation template](https://github.com/CatalystCode/ibex-dashboard/blob/master/server/dashboards/preconfigured/bot-framework-inst.ts) which requires you to enter your Application Insights App Id and App Key. Also depending on your bot you will need to add [Node.js instrumentation](https://github.com/CatalystCode/botbuilder-instrumentation) or [C# instrumentation](https://github.com/CatalystCode/botbuilder-instrumentation-cs) in order to enable logging to Application Insights. Then after a couple of minutes you will start to see the data come through! The dashboard can be completely customised using [generic components](https://github.com/CatalystCode/ibex-dashboard/tree/master/client/src/components/generic) including charts, tables, score cards and drill-down dialogs. These elements can be used to review how your bot performs over time, monitor usage stats, message sentiment, user retention and inspect user intents.

[![]({{ site.baseurl }}/assets/images/Ibex-Dashboard-Bot-Analytics.png)](https://github.com/CatalystCode/ibex-dashboard)

If you are new to [Application Insights](https://azure.microsoft.com/en-gb/services/application-insights/) one of the useful features of the [Ibex dashboard](https://github.com/CatalystCode/ibex-dashboard) is the ability to inspect an element's Application Insights query and the formatted JSON data side by side. 

[![]({{ site.baseurl }}/assets/images/Ibex-Dashboard-Bot-Analytics-App-Insights-Query.png)](https://github.com/CatalystCode/ibex-dashboard)

This query can be copied and played back inside your Application Insights live code editor. This is a good way to learn how the Application Insights queries work as you can step through the query by commenting out various lines with double slashes '//'.

[![]({{ site.baseurl }}/assets/images/Application-Insights-Query.png)](https://github.com/CatalystCode/ibex-dashboard)

## Writing Azure Log Analytics queries for Ibex dashboard

The [Ibex dashboard schema](https://github.com/CatalystCode/ibex-dashboard/blob/master/docs/DASHBOARD-SCHEMA.md) is composed of meta data, data sources, filters, elements and dialogs. Each data source allows you to define a query and a 'calculated' javascript function to process the query's results for display purposes. Before learning to write Application Insights queries I was used to writing javascript map / reduce functions to aggregate data and so it's all too easy to rely on previous javascript knowledge to process the data from a basic query. But often this javascript 'reduce' aggregation logic can done in an Application Insights query with a lot less effort. So invest some time up front to [learn the key Application Insights query concepts](https://docs.loganalytics.io/docs/Learn/Getting-Started/Getting-started-with-the-Analytics-portal) and it will pay off in the long run!

To help to illustrate this we can look at the Application Insights query for tracking a bot to human hand-off during a user's conversation session. For this scenario we built a [QnA bot](https://qnamaker.ai/) with the [hand-off module](https://www.npmjs.com/package/botbuilder-handoff) installed. If a customer asks the QnA bot a question and no answer was found in the knowledge base we trigger an automatic hand-off to human. We want to show the fastest, longest and average times for customer waiting for an human agent to respond in the dashboard.

We can start by writing a basic query in Application Insights to get all the transcripts from the 'customEvents' table and 'project' only the information we need.

```js
customEvents 
where name == 'Transcript'
| extend customerName=tostring(customDimensions.customerName), 
  text=tostring(customDimensions.text), 
  userTime=tostring(customDimensions.timestamp), 
  state=toint(customDimensions.state), 
  agentName=tostring(customDimensions.agentName), 
  from=tostring(customDimensions.from)  
| project from, text, customerName, agentName, state, userTime 
| order by userTime asc
```

But in this example we are not using Application Insights to aggregate the results so we end up with a lot of results to process. Given the query above the following code snippet is the amount of Javascript required.

```js
calculated: (transcripts) =&gt; {
  const key = 'customerId';
  const transcriptsGrouped = transcripts.reduce((a, c) =&gt; {
    const i = a.findIndex(col =&gt; col.id === c[key]);
    if (i === -1) {
      let collection = {
        'id': c[key],
        'transcripts': []
      };
      a.push(collection); // new group
    } else {
      a[i].transcripts.push(c); // append to group
    }
    return a;
  }, []);

  const SEC_PER_DAY = 86400; // 60 * 60 * 24;
  let times = [];

  transcriptsGrouped.forEach(userTranscript =&gt; {
    let prevTranscript = null;
    userTranscript.transcripts.forEach(transcript =&gt; {
      if (prevTranscript &amp;&amp; prevTranscript.state === 1 &amp;&amp; transcript.state === 2) {
        let date1 = new Date(prevTranscript.userTime);
        let date2 = new Date(transcript.userTime);
        let diff = (date2 - date1) / SEC_PER_DAY;
        times.push(diff);
      }
      prevTranscript = transcript;
    });
  });

  const avgTimeWaiting = times.reduce((a, c) =&gt; { return a + c }, 0) / times.length;
  const maxTimeWaiting = Math.max(...times);
  const minTimeWaiting = Math.min(...times);

  return {
    'transcriptsAverageTimeWaiting-value': avgTimeWaiting.toFixed(2),
    'transcriptsLongestTimeWaiting-value': maxTimeWaiting.toFixed(2),
    'transcriptsShortestTimeWaiting-value': minTimeWaiting.toFixed(2),
  };
}
```

The first 'reduce' block is required to group the transcripts per user Id. Then for every user we track the state change from waiting and talking to human agent and calculate the time difference in seconds. Where 'state' is an integer value that marks the current status of the conversion.  
`
0 = Bot
1 = Waiting
2 = Human agent
`

But we can optimise the code by doing the aggregation within the Application Insights query by using the 'summarize' operator and 'count' function.

```js
customEvents
where name == 'Transcript'
| extend conversationId=tostring(customDimensions.userConversationId), 
  customerId=tostring(customDimensions.customerId), 
  state=toint(customDimensions.state)  
| where state==1 or state==2
| order by timestamp asc
| summarize total=count(), times=makelist(timestamp) by conversationId, customerId, bin(state, 1)
| project conversationId, customerId, state, startTime=times[0] 
| summarize result=count(state), startEndTimes=makelist(startTime) by conversationId, customerId
| where result == 2
| project conversationId, customerId, timeTaken=todatetime(startEndTimes[1])-todatetime(startEndTimes[0])
```

Notice how you can apply aggregations in multiple passes, in this case the 'summarize' operator and 'count' function is used to aggregate results twice in conjunction with multiple 'where' statements that are used to filter the results. Now the javascript 'calculated' function code can be greatly simplified:

```js
calculated: (results) =&gt; {
  const times = results.reduce(
    (acc, cur) =&gt; {
      // converts time hh:mm:ss format to value in seconds
      acc.push(cur.timeTaken.split(':').reverse().reduce((a, c, i) =&gt; a + c * Math.pow(60, i), 0));
      return acc;
    },
    []);

  const avgTimeWaiting = times.reduce((a, c) =&gt; a + c, 0) / times.length;
  const maxTimeWaiting = Math.max(...times);
  const minTimeWaiting = Math.min(...times);

  return {
    'transcriptsAverageTimeWaiting-value': avgTimeWaiting.toFixed(2),
    'transcriptsLongestTimeWaiting-value': maxTimeWaiting.toFixed(2),
    'transcriptsShortestTimeWaiting-value': minTimeWaiting.toFixed(2),
  };
}
```

The only thing we do is a 'reduce' function to convert the time format 'hh:mm:ss' returned from the Application Insights query into a number of seconds for the various calculations for displaying in a score card element.

The final Application Insights query is available in the [hand-off to human dashboard template](https://github.com/CatalystCode/ibex-dashboard/blob/master/server/dashboards/preconfigured/human-handoff.ts) and is included with [Ibex dashboard](https://github.com/CatalystCode/ibex-dashboard).

#### Further reading and resources:

- [Azure Application Insights](https://docs.microsoft.com/en-us/azure/application-insights/app-insights-analytics)
- [Azure Log Analytics query language](https://docs.loganalytics.io/)
- [Azure Log Analytics query language reference docs](https://docs.loganalytics.io/docs/Language-Reference/)
- [Azure Log Analytics playground](http://portal.loganalytics.io/demo)
- [Ibex dashboard for Application Insights](https://github.com/CatalystCode/ibex-dashboard)</content><author><name>David Douglas</name></author><category term="Application Insights" /><category term="Ibex Dashboard" /><category term="Data Visualisation" /><category term="Bots" /><category term="Log Analytics" /><summary type="html">Ibex dashboard is an open source web app for displaying telemetry data from Application Insights. It comes with a number of sample templates including analytics dashboards for Bots. If you’re developing a bot and you want to see how your bot is performing over time then you can select the Bot Instrumentation template which requires you to enter your Application Insights App Id and App Key. Also depending on your bot you will need to add Node.js instrumentation or C# instrumentation in order to enable logging to Application Insights. Then after a couple of minutes you will start to see the data come through! The dashboard can be completely customised using generic components including charts, tables, score cards and drill-down dialogs. These elements can be used to review how your bot performs over time, monitor usage stats, message sentiment, user retention and inspect user intents. If you are new to Application Insights one of the useful features of the Ibex dashboard is the ability to inspect an element’s Application Insights query and the formatted JSON data side by side. This query can be copied and played back inside your Application Insights live code editor. This is a good way to learn how the Application Insights queries work as you can step through the query by commenting out various lines with double slashes ‘//’. Writing Azure Log Analytics queries for Ibex dashboard The Ibex dashboard schema is composed of meta data, data sources, filters, elements and dialogs. Each data source allows you to define a query and a ‘calculated’ javascript function to process the query’s results for display purposes. Before learning to write Application Insights queries I was used to writing javascript map / reduce functions to aggregate data and so it’s all too easy to rely on previous javascript knowledge to process the data from a basic query. But often this javascript ‘reduce’ aggregation logic can done in an Application Insights query with a lot less effort. So invest some time up front to learn the key Application Insights query concepts and it will pay off in the long run! To help to illustrate this we can look at the Application Insights query for tracking a bot to human hand-off during a user’s conversation session. For this scenario we built a QnA bot with the hand-off module installed. If a customer asks the QnA bot a question and no answer was found in the knowledge base we trigger an automatic hand-off to human. We want to show the fastest, longest and average times for customer waiting for an human agent to respond in the dashboard. We can start by writing a basic query in Application Insights to get all the transcripts from the ‘customEvents’ table and ‘project’ only the information we need. customEvents where name == 'Transcript' | extend customerName=tostring(customDimensions.customerName), text=tostring(customDimensions.text), userTime=tostring(customDimensions.timestamp), state=toint(customDimensions.state), agentName=tostring(customDimensions.agentName), from=tostring(customDimensions.from) | project from, text, customerName, agentName, state, userTime | order by userTime asc But in this example we are not using Application Insights to aggregate the results so we end up with a lot of results to process. Given the query above the following code snippet is the amount of Javascript required. calculated: (transcripts) =&amp;gt; { const key = 'customerId'; const transcriptsGrouped = transcripts.reduce((a, c) =&amp;gt; { const i = a.findIndex(col =&amp;gt; col.id === c[key]); if (i === -1) { let collection = { 'id': c[key], 'transcripts': [] }; a.push(collection); // new group } else { a[i].transcripts.push(c); // append to group } return a; }, []); const SEC_PER_DAY = 86400; // 60 * 60 * 24; let times = []; transcriptsGrouped.forEach(userTranscript =&amp;gt; { let prevTranscript = null; userTranscript.transcripts.forEach(transcript =&amp;gt; { if (prevTranscript &amp;amp;&amp;amp; prevTranscript.state === 1 &amp;amp;&amp;amp; transcript.state === 2) { let date1 = new Date(prevTranscript.userTime); let date2 = new Date(transcript.userTime); let diff = (date2 - date1) / SEC_PER_DAY; times.push(diff); } prevTranscript = transcript; }); }); const avgTimeWaiting = times.reduce((a, c) =&amp;gt; { return a + c }, 0) / times.length; const maxTimeWaiting = Math.max(...times); const minTimeWaiting = Math.min(...times); return { 'transcriptsAverageTimeWaiting-value': avgTimeWaiting.toFixed(2), 'transcriptsLongestTimeWaiting-value': maxTimeWaiting.toFixed(2), 'transcriptsShortestTimeWaiting-value': minTimeWaiting.toFixed(2), }; } The first ‘reduce’ block is required to group the transcripts per user Id. Then for every user we track the state change from waiting and talking to human agent and calculate the time difference in seconds. Where ‘state’ is an integer value that marks the current status of the conversion. ` 0 = Bot 1 = Waiting 2 = Human agent ` But we can optimise the code by doing the aggregation within the Application Insights query by using the ‘summarize’ operator and ‘count’ function. customEvents where name == 'Transcript' | extend conversationId=tostring(customDimensions.userConversationId), customerId=tostring(customDimensions.customerId), state=toint(customDimensions.state) | where state==1 or state==2 | order by timestamp asc | summarize total=count(), times=makelist(timestamp) by conversationId, customerId, bin(state, 1) | project conversationId, customerId, state, startTime=times[0] | summarize result=count(state), startEndTimes=makelist(startTime) by conversationId, customerId | where result == 2 | project conversationId, customerId, timeTaken=todatetime(startEndTimes[1])-todatetime(startEndTimes[0]) Notice how you can apply aggregations in multiple passes, in this case the ‘summarize’ operator and ‘count’ function is used to aggregate results twice in conjunction with multiple ‘where’ statements that are used to filter the results. Now the javascript ‘calculated’ function code can be greatly simplified: calculated: (results) =&amp;gt; { const times = results.reduce( (acc, cur) =&amp;gt; { // converts time hh:mm:ss format to value in seconds acc.push(cur.timeTaken.split(':').reverse().reduce((a, c, i) =&amp;gt; a + c * Math.pow(60, i), 0)); return acc; }, []); const avgTimeWaiting = times.reduce((a, c) =&amp;gt; a + c, 0) / times.length; const maxTimeWaiting = Math.max(...times); const minTimeWaiting = Math.min(...times); return { 'transcriptsAverageTimeWaiting-value': avgTimeWaiting.toFixed(2), 'transcriptsLongestTimeWaiting-value': maxTimeWaiting.toFixed(2), 'transcriptsShortestTimeWaiting-value': minTimeWaiting.toFixed(2), }; } The only thing we do is a ‘reduce’ function to convert the time format ‘hh:mm:ss’ returned from the Application Insights query into a number of seconds for the various calculations for displaying in a score card element. The final Application Insights query is available in the hand-off to human dashboard template and is included with Ibex dashboard. Further reading and resources: Azure Application Insights Azure Log Analytics query language Azure Log Analytics query language reference docs Azure Log Analytics playground Ibex dashboard for Application Insights</summary></entry><entry><title type="html">Unity3d and Azure Blob Storage</title><link href="/tutorial/unity3d-and-azure-blob-storage" rel="alternate" type="text/html" title="Unity3d and Azure Blob Storage" /><published>2017-03-10T12:14:11+01:00</published><updated>2017-03-10T12:14:11+01:00</updated><id>/tutorial/unity3d-and-azure-blob-storage</id><content type="html" xml:base="/tutorial/unity3d-and-azure-blob-storage">Previously I've looked at using [Azure App Services for Unity](http://www.deadlyfingers.net/azure/azure-app-services-for-unity3d/), which provided a backend for Unity applications or games using Easy Tables and Easy APIs. But what if I wanted to lift and shift heavier data such as audio files, image files, or Unity Asset Bundles binaries? For storing these types of files, I would be better using [Azure Blob storage](https://azure.microsoft.com/en-gb/services/storage/blobs/). Recently I created an [Azure Blob storage demo project in Unity](https://github.com/Unity3dAzure/StorageServicesDemo) to show how to save and load these various asset types in Unity. One of the exciting new applications for Unity is developing &lt;acronym title=&quot;Virtual Reality&quot;&gt;VR&lt;/acronym&gt;, &lt;acronym title=&quot;Augmented Reality&quot;&gt;AR&lt;/acronym&gt; or &lt;acronym title=&quot;Mixed Reality&quot;&gt;MR&lt;/acronym&gt; experiences for HoloLens where a backend could serve media content dynamically whether it's images, audio, or prefabs with models, materials and referenced scripts. When thinking of cloud gaming the tendency is to consider it in terms of end user scenarios like massive multiplayer online games. While Azure is designed to scale, it is also helpful to use during early stage development and testing. There is an opportunity to create productive cloud tools for artists, designers and developers especially when extensive hardware testing is required in Virtual Reality, Augmented Reality or Mixed Reality development. For example, imagine being able to see and test updates on the hardware without having to rebuild the binaries in Unity or Visual Studio each time. There are many more use cases than IΓÇÖve mentioned here like offering user generated downloadable content for extending your game or app.

I'll be covering the load and save code snippets from the [Unity and Azure Blob storage demo commentary](https://youtu.be/0gpg2xwusjM) which you can watch to see how you can save and load image textures, audio clips as _.wav_ files, and Asset Bundles. The Unity Asset Bundle demo will also include loading Prefabs and dynamically adding them into a Unity Scene using XML or JSON data which should give you some ideas of how you might like to use Blob storage in your Unity development or end user scenario.

&lt;div class=&quot;video&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/0gpg2xwusjM?ecver=2&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

## Setup Azure Blob Storage

Setting up Blob Storage for the Unity demo can be done quickly in just a couple of steps:

1. Sign in to your [Azure portal](https://portal.azure.com) and create a new Storage Account.
 ![01-StorageAccount]({{ site.baseurl }}/assets/images/01-StorageAccount.png)
2. Once the Storage account is provisioned then select the add new container button which will be used for storing the blobs.

 ![02-CreateContainer]({{ site.baseurl }}/assets/images/02-CreateContainer.png)
3. Create the '&lt;string&gt;Blob' type container which permits public read access for the purposes of this demo.&lt;/string&gt;

 ![03-NewContainer-BlobAccess]({{ site.baseurl }}/assets/images/03-NewContainer-BlobAccess.png)

## Audio files

### Saving Unity Audio Clips into Blob Storage

For the [Unity audio blob demo](https://github.com/Unity3dAzure/StorageServicesDemo/tree/master/Assets/Demos/Audio) I created a [helper script to convert Unity Audio Clip recording to .wav files](https://github.com/deadlyfingers/UnityWav) for the purpose of saving to Azure Blob Storage.  
Once the audio has been recorded in Unity I can upload the file using the `PutAudioAudio` method which takes a callback function, the wav bytes, the container resource path, the filename and the file's mime type. By the way this method must be wrapped using [StartCoroutine](https://docs.unity3d.com/ScriptReference/MonoBehaviour.StartCoroutine.html) which is the way Unity 5 handles asynchronous requests. Once the request is completed it will trigger the `PutAudioCompleted` callback function I have provided my script with a response object. If the response is successful you will see the wav file blob added in your Blob Container.

☞ Tip: Grab the [Storage Explorer app](http://storageexplorer.com/) for viewing all the blobs!

```csharp
private void PutAudio ()
{
	byte[] wavBytes = File.ReadAllBytes (localPath);
	string filename = Path.GetFileName (localPath);
	Debug.Log (&quot;Put audio file: &quot; + filename);
	StartCoroutine (blobService.PutAudioBlob (PutAudioCompleted, wavBytes, container, filename, &quot;audio/wav&quot;));
}

public void PutAudioCompleted (RestResponse response)
{
	if (response.IsError) {
		Debug.LogError( &quot;Put audio file error: &quot; + response.ErrorMessage );
		return;
	}
	Debug.Log (&quot;Put audio blob success: &quot; + response.Url);
}
```

### Loading .wav files from Blob Storage

As we used the Blob type container with public read access you can use the [`UnityWebRequest.GetAudioClip`](https://docs.unity3d.com/ScriptReference/Networking.UnityWebRequest.GetAudioClip.html) method to directly load the .wav file from Azure Blob Storage and handle it as a native Unity AudioClip type for playback.

```csharp
public void TappedLoad ()
{
	string filename = Path.GetFileName (localPath);
	string url = Path.Combine (client.PrimaryEndpoint () + container, filename);
	Debug.Log (&quot;Load audio blob: &quot; + url);
	StartCoroutine (LoadAudioURL (url));
}

private IEnumerator LoadAudioURL (string url)
{
	UnityWebRequest www = UnityWebRequest.GetAudioClip (url, AudioType.WAV);
	yield return www.Send ();
	if (www.isError) {
		Debug.LogError( &quot;Load audio url error: &quot; + www.error );
	} else {
		AudioClip audioClip = ((DownloadHandlerAudioClip)www.downloadHandler).audioClip;
		audioSource.clip = audioClip;
		audioSource.Play ();
	}
}
```

## Image files

For the [Unity image blob demo](https://github.com/Unity3dAzure/StorageServicesDemo/tree/master/Assets/Demos/Image) I used Unity's [`Application.CaptureScreenshot`](https://docs.unity3d.com/ScriptReference/Application.CaptureScreenshot.html) method to generate a png image representation of the current state of the game screen.

### Saving Images into Blob Storage

The image is saved using the `PutImageBlob` method which is similar to the audio blob except we pass the image bytes and mime type.

```csharp
private void PutImage (byte[] imageBytes)
{
	string filename = Path.GetFileName (localPath);
	StartCoroutine (blobService.PutImageBlob (PutImageCompleted, imageBytes, container, filename, &quot;image/png&quot;));
}

private void PutImageCompleted (RestResponse response)
{
	if (response.IsError) {
		Debug.LogError( &quot;Put image file error: &quot; + response.ErrorMessage );
		return;
	}
	Debug.Log (&quot;Put image blob:&quot; + response.Url);
}
```

### Loading Image Textures from Blob Storage

As we used the Blob type container with public read access you can use the [`UnityWebRequest.GetTexture`](https://docs.unity3d.com/ScriptReference/Networking.UnityWebRequest.GetTexture.html) method to directly load the .png file from Azure Blob Storage and handle it as a native Unity Texture type for use. As I want to use the Texture in Unity UI to display as an [Image](https://docs.unity3d.com/ScriptReference/UI.Image.html) I need to convert it to a sprite using my `ChangeImage` function.

```csharp
public void TappedLoad ()
{
	ChangeImage (new Texture2D (1, 1));
	string filename = Path.GetFileName (localPath);
	string url = Path.Combine (client.PrimaryEndpoint () + container, filename);
	Debug.Log (&quot;Load image: &quot; + url);
	StartCoroutine (LoadImageURL (url));
}

public IEnumerator LoadImageURL (string url)
{
	UnityWebRequest www = UnityWebRequest.GetTexture (url);
	yield return www.Send ();
	if (www.isError) {
		Debug.LogError( &quot;Load image url error: &quot; + www.error );
	} else {
		Texture texture = ((DownloadHandlerTexture)www.downloadHandler).texture;
		ChangeImage (texture);
	}
}

private void ChangeImage (Texture2D texture)
{
	Sprite sprite = Sprite.Create (texture, new Rect (0, 0, texture.width, texture.height), Vector2.zero);
	image.GetComponent&lt;Image&gt; ().sprite = sprite;
}

private void ChangeImage (Texture texture)
{
	ChangeImage (texture as Texture2D);
}
```

## Unity Asset Bundles

[Unity Asset Bundles](https://docs.unity3d.com/ScriptReference/AssetBundle.html) provide a way to dynamically load in assets in your project. This [Asset Bundle demo for Blob Storage](https://github.com/Unity3dAzure/StorageServicesDemo/tree/master/Assets/Demos/AssetBundle) is a little more complicated than the other examples. An important note to remember is that Asset Bundle binaries need to be build for each target platform. Refer to Unity documentation on [building Asset Bundles](https://docs.unity3d.com/Manual/BuildingAssetBundles.html) for more info on building Asset Bundles. Also make sure to review the code stripping section if you want to be able to use referenced scripts in your Prefabs when you do a build.

### Building and uploading the Asset Bundles for each platform to Blob Storage

I have included the Editor scripts with the demo to build the Asset Bundle for each platform. NB: Windows 10 Store App (or HoloLens) bundles can only be built on the Windows Unity Editor at time of writing this. Building the Asset Bundles and uploading them is performed inside Unity Editor:

1. Select Assets \&gt; Build Asset Bundles
2. Select Window \&gt; Upload Asset Bundles...

### Loading Asset Bundles from Blob Storage

```csharp
public void TappedLoadAssetBundle ()
{
	string filename = assetBundleName + &quot;-&quot; + GetAssetBundlePlatformName () + &quot;.unity3d&quot;;
	string url = Path.Combine (client.SecondaryEndpoint () + container, filename);
	Debug.Log (&quot;Load asset bundle: &quot; + url);
	StartCoroutine (LoadAssetBundleURL (url));
}

public IEnumerator LoadAssetBundleURL (string url)
{
	UnityWebRequest www = UnityWebRequest.GetAssetBundle (url);
	yield return www.Send ();
	if (www.isError) {
		Debug.LogError( &quot;Load Asset Bundle url error: &quot; + www.error );
		yield break;
	} else {
		assetBundle = ((DownloadHandlerAssetBundle)www.downloadHandler).assetBundle;
		Debug.Log(&quot;Load url: &quot; + url);
		StartCoroutine (LoadAssets (assetBundle, &quot;CloudCube&quot;));
	}
}

private string GetAssetBundlePlatformName ()
{
	switch (Application.platform) {
	case RuntimePlatform.WindowsEditor:
	case RuntimePlatform.WindowsPlayer:
		return SystemInfo.operatingSystem.Contains (&quot;64 bit&quot;) ? &quot;x64&quot; : &quot;x86&quot;;
	case RuntimePlatform.WSAPlayerX86:
	case RuntimePlatform.WSAPlayerX64:
	case RuntimePlatform.WSAPlayerARM:
		return &quot;WSA&quot;;
	case RuntimePlatform.Android:
		return &quot;Android&quot;;
	case RuntimePlatform.IPhonePlayer:
		return &quot;iOS&quot;;
	case RuntimePlatform.OSXEditor:
	case RuntimePlatform.OSXPlayer:
		return &quot;OSX&quot;;
	default:
		throw new Exception (&quot;Platform not listed&quot;);
	}
}
```

If you like the [Azure Storage Services library for Unity](https://github.com/Unity3dAzure/StorageServices) let me know about it on [Twitter](https://twitter.com/deadlyfingers). Any issues, features or blob storage demo requests please create it as an issue on github for others to learn from and collaborate.</content><author><name>David Douglas</name></author><category term="Azure" /><category term="Blob Storage" /><category term="Unity3D" /><summary type="html">Previously I’ve looked at using Azure App Services for Unity, which provided a backend for Unity applications or games using Easy Tables and Easy APIs. But what if I wanted to lift and shift heavier data such as audio files, image files, or Unity Asset Bundles binaries? For storing these types of files, I would be better using Azure Blob storage. Recently I created an Azure Blob storage demo project in Unity to show how to save and load these various asset types in Unity. One of the exciting new applications for Unity is developing VR, AR or MR experiences for HoloLens where a backend could serve media content dynamically whether it’s images, audio, or prefabs with models, materials and referenced scripts. When thinking of cloud gaming the tendency is to consider it in terms of end user scenarios like massive multiplayer online games. While Azure is designed to scale, it is also helpful to use during early stage development and testing. There is an opportunity to create productive cloud tools for artists, designers and developers especially when extensive hardware testing is required in Virtual Reality, Augmented Reality or Mixed Reality development. For example, imagine being able to see and test updates on the hardware without having to rebuild the binaries in Unity or Visual Studio each time. There are many more use cases than IΓÇÖve mentioned here like offering user generated downloadable content for extending your game or app. I’ll be covering the load and save code snippets from the Unity and Azure Blob storage demo commentary which you can watch to see how you can save and load image textures, audio clips as .wav files, and Asset Bundles. The Unity Asset Bundle demo will also include loading Prefabs and dynamically adding them into a Unity Scene using XML or JSON data which should give you some ideas of how you might like to use Blob storage in your Unity development or end user scenario. Setup Azure Blob Storage Setting up Blob Storage for the Unity demo can be done quickly in just a couple of steps: Sign in to your Azure portal and create a new Storage Account. Once the Storage account is provisioned then select the add new container button which will be used for storing the blobs. Create the ‘Blob' type container which permits public read access for the purposes of this demo. Audio files Saving Unity Audio Clips into Blob Storage For the Unity audio blob demo I created a helper script to convert Unity Audio Clip recording to .wav files for the purpose of saving to Azure Blob Storage. Once the audio has been recorded in Unity I can upload the file using the PutAudioAudio method which takes a callback function, the wav bytes, the container resource path, the filename and the file’s mime type. By the way this method must be wrapped using StartCoroutine which is the way Unity 5 handles asynchronous requests. Once the request is completed it will trigger the PutAudioCompleted callback function I have provided my script with a response object. If the response is successful you will see the wav file blob added in your Blob Container. ☞ Tip: Grab the Storage Explorer app for viewing all the blobs! private void PutAudio () { byte[] wavBytes = File.ReadAllBytes (localPath); string filename = Path.GetFileName (localPath); Debug.Log (&quot;Put audio file: &quot; + filename); StartCoroutine (blobService.PutAudioBlob (PutAudioCompleted, wavBytes, container, filename, &quot;audio/wav&quot;)); } public void PutAudioCompleted (RestResponse response) { if (response.IsError) { Debug.LogError( &quot;Put audio file error: &quot; + response.ErrorMessage ); return; } Debug.Log (&quot;Put audio blob success: &quot; + response.Url); } Loading .wav files from Blob Storage As we used the Blob type container with public read access you can use the UnityWebRequest.GetAudioClip method to directly load the .wav file from Azure Blob Storage and handle it as a native Unity AudioClip type for playback. public void TappedLoad () { string filename = Path.GetFileName (localPath); string url = Path.Combine (client.PrimaryEndpoint () + container, filename); Debug.Log (&quot;Load audio blob: &quot; + url); StartCoroutine (LoadAudioURL (url)); } private IEnumerator LoadAudioURL (string url) { UnityWebRequest www = UnityWebRequest.GetAudioClip (url, AudioType.WAV); yield return www.Send (); if (www.isError) { Debug.LogError( &quot;Load audio url error: &quot; + www.error ); } else { AudioClip audioClip = ((DownloadHandlerAudioClip)www.downloadHandler).audioClip; audioSource.clip = audioClip; audioSource.Play (); } } Image files For the Unity image blob demo I used Unity’s Application.CaptureScreenshot method to generate a png image representation of the current state of the game screen. Saving Images into Blob Storage The image is saved using the PutImageBlob method which is similar to the audio blob except we pass the image bytes and mime type. private void PutImage (byte[] imageBytes) { string filename = Path.GetFileName (localPath); StartCoroutine (blobService.PutImageBlob (PutImageCompleted, imageBytes, container, filename, &quot;image/png&quot;)); } private void PutImageCompleted (RestResponse response) { if (response.IsError) { Debug.LogError( &quot;Put image file error: &quot; + response.ErrorMessage ); return; } Debug.Log (&quot;Put image blob:&quot; + response.Url); } Loading Image Textures from Blob Storage As we used the Blob type container with public read access you can use the UnityWebRequest.GetTexture method to directly load the .png file from Azure Blob Storage and handle it as a native Unity Texture type for use. As I want to use the Texture in Unity UI to display as an Image I need to convert it to a sprite using my ChangeImage function. public void TappedLoad () { ChangeImage (new Texture2D (1, 1)); string filename = Path.GetFileName (localPath); string url = Path.Combine (client.PrimaryEndpoint () + container, filename); Debug.Log (&quot;Load image: &quot; + url); StartCoroutine (LoadImageURL (url)); } public IEnumerator LoadImageURL (string url) { UnityWebRequest www = UnityWebRequest.GetTexture (url); yield return www.Send (); if (www.isError) { Debug.LogError( &quot;Load image url error: &quot; + www.error ); } else { Texture texture = ((DownloadHandlerTexture)www.downloadHandler).texture; ChangeImage (texture); } } private void ChangeImage (Texture2D texture) { Sprite sprite = Sprite.Create (texture, new Rect (0, 0, texture.width, texture.height), Vector2.zero); image.GetComponent&amp;lt;Image&amp;gt; ().sprite = sprite; } private void ChangeImage (Texture texture) { ChangeImage (texture as Texture2D); } Unity Asset Bundles Unity Asset Bundles provide a way to dynamically load in assets in your project. This Asset Bundle demo for Blob Storage is a little more complicated than the other examples. An important note to remember is that Asset Bundle binaries need to be build for each target platform. Refer to Unity documentation on building Asset Bundles for more info on building Asset Bundles. Also make sure to review the code stripping section if you want to be able to use referenced scripts in your Prefabs when you do a build. Building and uploading the Asset Bundles for each platform to Blob Storage I have included the Editor scripts with the demo to build the Asset Bundle for each platform. NB: Windows 10 Store App (or HoloLens) bundles can only be built on the Windows Unity Editor at time of writing this. Building the Asset Bundles and uploading them is performed inside Unity Editor: Select Assets &amp;gt; Build Asset Bundles Select Window &amp;gt; Upload Asset Bundles… Loading Asset Bundles from Blob Storage public void TappedLoadAssetBundle () { string filename = assetBundleName + &quot;-&quot; + GetAssetBundlePlatformName () + &quot;.unity3d&quot;; string url = Path.Combine (client.SecondaryEndpoint () + container, filename); Debug.Log (&quot;Load asset bundle: &quot; + url); StartCoroutine (LoadAssetBundleURL (url)); } public IEnumerator LoadAssetBundleURL (string url) { UnityWebRequest www = UnityWebRequest.GetAssetBundle (url); yield return www.Send (); if (www.isError) { Debug.LogError( &quot;Load Asset Bundle url error: &quot; + www.error ); yield break; } else { assetBundle = ((DownloadHandlerAssetBundle)www.downloadHandler).assetBundle; Debug.Log(&quot;Load url: &quot; + url); StartCoroutine (LoadAssets (assetBundle, &quot;CloudCube&quot;)); } } private string GetAssetBundlePlatformName () { switch (Application.platform) { case RuntimePlatform.WindowsEditor: case RuntimePlatform.WindowsPlayer: return SystemInfo.operatingSystem.Contains (&quot;64 bit&quot;) ? &quot;x64&quot; : &quot;x86&quot;; case RuntimePlatform.WSAPlayerX86: case RuntimePlatform.WSAPlayerX64: case RuntimePlatform.WSAPlayerARM: return &quot;WSA&quot;; case RuntimePlatform.Android: return &quot;Android&quot;; case RuntimePlatform.IPhonePlayer: return &quot;iOS&quot;; case RuntimePlatform.OSXEditor: case RuntimePlatform.OSXPlayer: return &quot;OSX&quot;; default: throw new Exception (&quot;Platform not listed&quot;); } } If you like the Azure Storage Services library for Unity let me know about it on Twitter. Any issues, features or blob storage demo requests please create it as an issue on github for others to learn from and collaborate.</summary></entry><entry><title type="html">Merging Unity scenes, prefabs and assets with git</title><link href="/info/unity-git" rel="alternate" type="text/html" title="Merging Unity scenes, prefabs and assets with git" /><published>2016-09-19T10:30:38+01:00</published><updated>2016-09-19T10:30:38+01:00</updated><id>/info/unity-git</id><content type="html" xml:base="/info/unity-git">When it comes to working as a team on the same project we are all thankful for source control. But even if you're cool with [git](https://git-scm.com/) there are some things to be aware of when starting new source controlled Unity projects that should help to reduce the chance of nasty merge conflicts.

## Solo Scenes

Something to generally avoid in Unity is working on the same scene. Thats why the question of [how to merge a scene when a team of developers are working on it](http://forum.unity3d.com/threads/how-do-several-people-work-inside-the-same-scene-and-resolve-merging-conflicts-under-git.291468/) is a fairly hot topic. One basic strategy is for each person to clone the main scene and work on their own version, then nominate a scene master to combine the various elements into in the main scene to avoid conflicts. But because this is quite a restricted way of working Unity 5 introduced [Smart Merge](https://docs.unity3d.com/Manual/SmartMerge.html) and the **UnityYAMLMerge** tool that can merge scenes and prefabs semantically.

## Asset Serialization using &quot;Force Text&quot;

By default Unity will save scenes and prefabs as binary files. But there is an option to force Unity to save scenes as [YAML text based files](https://docs.unity3d.com/Manual/TextSceneFormat.html) instead. This setting can be found under the **Edit \&gt; Project Settings \&gt; Editor** menu and then under **Asset Serialization Mode** choose **Force Text**.

 ![unity-edit-projectsettings-editor-assetserializationmode-forcetext]({{ site.baseurl }}/assets/images/Unity-Edit-ProjectSettings-Editor-AssetSerializationMode-ForceText.png)

But as this is not the default setting make sure when applying this mode that everyone else on the team is happy to switch.  
If you select &quot;Force Text&quot; to save files in YAML format you should add a _.gitattributes_ file that tells git to treat _\*.unity_, _\*.prefab_ and _\*.asset_ files as binary to ensure git doesn't try to merge scenes automatically. Paste the following into the _.gitconfig_ file inside your Unity project:

```
*.unity binary
*.prefab binary
*.asset binary
```

Another result of saving in text file mode is that you can see the changes in source control commits.

## Setting up UnityYAMLMerge with Git

You can access the **UnityYAMLMerge** tool from command line and also hook it up with version control software. Paste the following into the _.gitconfig_ file inside your Unity project:

#### UnityYAMLMerge (Windows):

```powershell
[merge]
tool = unityyamlmerge

[mergetool &quot;unityyamlmerge&quot;]
trustExitCode = false
cmd = 'C:\Program Files\Unity\Editor\Data\Tools\UnityYAMLMerge.exe' merge -p &quot;$BASE&quot; &quot;$REMOTE&quot; &quot;$LOCAL&quot; &quot;$MERGED&quot;
```

#### UnityYAMLMerge (Mac):

```powershell
[merge]
tool = unityyamlmerge

[mergetool &quot;unityyamlmerge&quot;]
trustExitCode = false
cmd = '/Applications/Unity/Unity.app/Contents/Tools/UnityYAMLMerge' merge -p &quot;$BASE&quot; &quot;$REMOTE&quot; &quot;$LOCAL&quot; &quot;$MERGED&quot;
```

## GitMerge for Unity

Worth a mention is the free [GitMerge tool for Unity](https://flashg.github.io/GitMerge-for-Unity/) for merging scene and prefabs inside Unity Editor but unfortunately [this editor plugin is currently broken in Unity 5](https://github.com/FlaShG/GitMerge-for-Unity/issues/17). Once you start merging and are in a git merge state you can resolve the conflicts inside the Unity app using [GitMerge Window for Unity](https://flashg.github.io/GitMerge-for-Unity/#instructions-start) which is opened via menu **Window \&gt; GitMerge**.

## Merging Unity C# script conflicts with P4Merge app

For merging conflicts I prefer to use the free [P4Merge](https://www.perforce.com/product/components/perforce-visual-merge-and-diff-tools) visual merge tool which is available for [Mac](https://www.perforce.com/downloads/register/helix?return_url=http://www.perforce.com/downloads/perforce/r15.2/bin.macosx107x86_64/P4V.dmg&amp;platform_family=MACINTOSH&amp;platform=OS%20X%2010.8%2B%20%28x64%29&amp;version=2015.2/1312139&amp;product_selected=Perforce&amp;edition_selected=helix&amp;product_name=Helix%20P4V:%20:%20Visual%20Client&amp;prod_num=9) and [Windows](https://www.perforce.com/downloads/register/helix?return_url=http://www.perforce.com/downloads/perforce/r15.2/bin.ntx64/p4vinst64.exe&amp;platform_family=WINDOWS&amp;platform=Windows%20%28x64%29&amp;version=2015.2/1312139&amp;product_selected=Perforce&amp;edition_selected=helix&amp;product_name=Helix%20P4V:%20:%20Visual%20Client&amp;prod_num=9). Here's how to hook up the P4Merge app as the **global** git merge tool when issuing the `git mergetool` command:

#### P4Merge (Windows):

```powershell
git config --global merge.tool p4merge
git config --global mergetool.p4merge.path 'C:/Program Files/Perforce/p4merge.exe'
```

#### P4Merge (Mac):

```powershell
# Setup p4merge as a visual mergetool
git config --global merge.tool p4mergetool
git config --global mergetool.p4mergetool.cmd &quot;/Applications/p4merge.app/Contents/MacOS/p4merge \&quot;$BASE\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$MERGED\&quot;

git config --global mergetool.p4mergetool.keepTemporaries false
git config --global mergetool.p4mergetool.trustExitCode false
git config --global mergetool.p4mergetool.keepBackup false
git config --global mergetool.p4mergetool.prompt false

# Setup p4merge as a visual diff tool
git config --global diff.tool p4mergetool
git config --global difftool.p4mergetool.cmd &quot;/Applications/p4merge.app/Contents/Resources/launchp4merge \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot;

# show updated git config
git config --global --list
```

## Setup a .gitignore file for Unity projects

First up there are certain Unity folders and files you don't want to include in the repo. Only 'Assets' and 'ProjectSettings' need to be included. Other Unity generated folders like 'Library', 'obj', 'Temp' should be added to the _.gitignore_ file. Or you can just copy the boilerplate Unity [.gitignore](https://www.gitignore.io/api/unity) file. I also suggest ignoring generated files like OS and source control temp files:

```conf
# Source control temp files
*.orig
# OS generated
.DS_Store
```

Unfortunately I made the over zealous mistake of adding all _\*.meta_ files to the _.gitignore_ file. At first this seemed like a good idea until the repo gets cloned and you end up with broken script and resource links in the Unity Editor scene. The [Unity source control documentation](https://docs.unity3d.com/Manual/ExternalVersionControlSystemSupport.html) mentions that these _.meta_ files should be added to source control. However I found that its only the meta files associated with resource files and scripts that are linked to a [GameObject](https://docs.unity3d.com/ScriptReference/GameObject.html) in the Unity Editor that are required. By using the [exclusion rule in gitignore](https://git-scm.com/docs/gitignore) I can limit it so the only _.meta_ files to be saved are those within the [Unity special folders](https://docs.unity3d.com/Manual/SpecialFolders.html) like: 'Prefabs', 'Resources', 'Scenes' as well as a 'Scripts' folder. So if you wish to limit the meta files just add the following rules to the _.gitignore_:

```conf
# Include only the .meta files in specific folders
*.meta
!Assets/[Ee]ditor/**/*.meta
!Assets/Editor Default Resources/**/*.meta
!Assets/[Gg]izmos/**/*.meta
!Assets/[Pp]refabs/**/*.meta
!Assets/[Rr]esources/**/*.meta
!Assets/[Ss]cenes/**/*.meta
!Assets/[Ss]cripts/**/*.meta
!Assets/Standard Assets/**/*.meta
!Assets/StreamingAssets/**/*.meta
```

For example if I import the [Azure AppServices library for Unity](https://github.com/Unity3dAzure/AppServices) by copying it into the _Assets/AppServices_ directory that would mean no meta files would be pushed in commits for this folder as it's outside the _Assets/Scripts_ folder. But what if I use a library that will be linked with GameObjects like [TSTableView](https://bitbucket.org/tacticsoft/tstableview) for example which attaches to a Unity UI Scroll View. Either I can drop the _TSTableView_ folder inside the _Assets/Scripts_ directory, or if you prefer to keep third party scripts outside as I do then you also need to add the _Assets/TSTableView_ directory to the list of exceptions in the _.gitignore_ file:

```conf
!Assets/TSTableView/**/*.meta
```

If you adopt this convention just be aware that every time you add third party [MonoBehaviour](https://docs.unity3d.com/ScriptReference/MonoBehaviour.html) script libraries outside the _Assets/Scripts_ folder then these directories will need to be added as _.gitignore_ exceptions to save the associated _.meta_ files.</content><author><name>David Douglas</name></author><category term="Git" /><category term="Unity3D" /><category term="Visual Studio" /><summary type="html">When it comes to working as a team on the same project we are all thankful for source control. But even if you’re cool with git there are some things to be aware of when starting new source controlled Unity projects that should help to reduce the chance of nasty merge conflicts. Solo Scenes Something to generally avoid in Unity is working on the same scene. Thats why the question of how to merge a scene when a team of developers are working on it is a fairly hot topic. One basic strategy is for each person to clone the main scene and work on their own version, then nominate a scene master to combine the various elements into in the main scene to avoid conflicts. But because this is quite a restricted way of working Unity 5 introduced Smart Merge and the UnityYAMLMerge tool that can merge scenes and prefabs semantically. Asset Serialization using “Force Text” By default Unity will save scenes and prefabs as binary files. But there is an option to force Unity to save scenes as YAML text based files instead. This setting can be found under the Edit &amp;gt; Project Settings &amp;gt; Editor menu and then under Asset Serialization Mode choose Force Text. But as this is not the default setting make sure when applying this mode that everyone else on the team is happy to switch. If you select “Force Text” to save files in YAML format you should add a .gitattributes file that tells git to treat *.unity, *.prefab and *.asset files as binary to ensure git doesn’t try to merge scenes automatically. Paste the following into the .gitconfig file inside your Unity project: *.unity binary *.prefab binary *.asset binary Another result of saving in text file mode is that you can see the changes in source control commits. Setting up UnityYAMLMerge with Git You can access the UnityYAMLMerge tool from command line and also hook it up with version control software. Paste the following into the .gitconfig file inside your Unity project: UnityYAMLMerge (Windows): [merge] tool = unityyamlmerge [mergetool &quot;unityyamlmerge&quot;] trustExitCode = false cmd = 'C:\Program Files\Unity\Editor\Data\Tools\UnityYAMLMerge.exe' merge -p &quot;$BASE&quot; &quot;$REMOTE&quot; &quot;$LOCAL&quot; &quot;$MERGED&quot; UnityYAMLMerge (Mac): [merge] tool = unityyamlmerge [mergetool &quot;unityyamlmerge&quot;] trustExitCode = false cmd = '/Applications/Unity/Unity.app/Contents/Tools/UnityYAMLMerge' merge -p &quot;$BASE&quot; &quot;$REMOTE&quot; &quot;$LOCAL&quot; &quot;$MERGED&quot; GitMerge for Unity Worth a mention is the free GitMerge tool for Unity for merging scene and prefabs inside Unity Editor but unfortunately this editor plugin is currently broken in Unity 5. Once you start merging and are in a git merge state you can resolve the conflicts inside the Unity app using GitMerge Window for Unity which is opened via menu Window &amp;gt; GitMerge. Merging Unity C# script conflicts with P4Merge app For merging conflicts I prefer to use the free P4Merge visual merge tool which is available for Mac and Windows. Here’s how to hook up the P4Merge app as the global git merge tool when issuing the git mergetool command: P4Merge (Windows): git config --global merge.tool p4merge git config --global mergetool.p4merge.path 'C:/Program Files/Perforce/p4merge.exe' P4Merge (Mac): # Setup p4merge as a visual mergetool git config --global merge.tool p4mergetool git config --global mergetool.p4mergetool.cmd &quot;/Applications/p4merge.app/Contents/MacOS/p4merge \&quot;$BASE\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$MERGED\&quot; git config --global mergetool.p4mergetool.keepTemporaries false git config --global mergetool.p4mergetool.trustExitCode false git config --global mergetool.p4mergetool.keepBackup false git config --global mergetool.p4mergetool.prompt false # Setup p4merge as a visual diff tool git config --global diff.tool p4mergetool git config --global difftool.p4mergetool.cmd &quot;/Applications/p4merge.app/Contents/Resources/launchp4merge \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; # show updated git config git config --global --list Setup a .gitignore file for Unity projects First up there are certain Unity folders and files you don’t want to include in the repo. Only ‘Assets’ and ‘ProjectSettings’ need to be included. Other Unity generated folders like ‘Library’, ‘obj’, ‘Temp’ should be added to the .gitignore file. Or you can just copy the boilerplate Unity .gitignore file. I also suggest ignoring generated files like OS and source control temp files: # Source control temp files *.orig # OS generated .DS_Store Unfortunately I made the over zealous mistake of adding all *.meta files to the .gitignore file. At first this seemed like a good idea until the repo gets cloned and you end up with broken script and resource links in the Unity Editor scene. The Unity source control documentation mentions that these .meta files should be added to source control. However I found that its only the meta files associated with resource files and scripts that are linked to a GameObject in the Unity Editor that are required. By using the exclusion rule in gitignore I can limit it so the only .meta files to be saved are those within the Unity special folders like: ‘Prefabs’, ‘Resources’, ‘Scenes’ as well as a ‘Scripts’ folder. So if you wish to limit the meta files just add the following rules to the .gitignore: # Include only the .meta files in specific folders *.meta !Assets/[Ee]ditor/**/*.meta !Assets/Editor Default Resources/**/*.meta !Assets/[Gg]izmos/**/*.meta !Assets/[Pp]refabs/**/*.meta !Assets/[Rr]esources/**/*.meta !Assets/[Ss]cenes/**/*.meta !Assets/[Ss]cripts/**/*.meta !Assets/Standard Assets/**/*.meta !Assets/StreamingAssets/**/*.meta For example if I import the Azure AppServices library for Unity by copying it into the Assets/AppServices directory that would mean no meta files would be pushed in commits for this folder as it’s outside the Assets/Scripts folder. But what if I use a library that will be linked with GameObjects like TSTableView for example which attaches to a Unity UI Scroll View. Either I can drop the TSTableView folder inside the Assets/Scripts directory, or if you prefer to keep third party scripts outside as I do then you also need to add the Assets/TSTableView directory to the list of exceptions in the .gitignore file: !Assets/TSTableView/**/*.meta If you adopt this convention just be aware that every time you add third party MonoBehaviour script libraries outside the Assets/Scripts folder then these directories will need to be added as .gitignore exceptions to save the associated .meta files.</summary></entry><entry><title type="html">Swift JSON parsing for iOS development</title><link href="/code/swift-json-parsing-for-ios-development" rel="alternate" type="text/html" title="Swift JSON parsing for iOS development" /><published>2016-08-23T09:37:01+01:00</published><updated>2016-08-23T09:37:01+01:00</updated><id>/code/swift-json-parsing-for-ios-development</id><content type="html" xml:base="/code/swift-json-parsing-for-ios-development">Recently I started a new iOS Swift project and spent way more time than I would like trying to find a JSON parser that could handle the various JSON data models I was working with. In this post I will document some real code samples that should prove useful for other iOS developers looking to get off to head start with data modelling in Swift.

## The search for a Swift JSON parser…

Handling JSON is a very common task with modern app development whether its consuming some REST Service API, loading a JSON file or document objects from database. With regards to Windows C# apps [Newtonsoft JSON](http://www.newtonsoft.com/json) is the popular choice and similarly with Java for Android there is [GSON](https://github.com/google/gson). But what library to use for iOS apps? Previously I had used libraries like [JSONModel](https://github.com/jsonmodel/jsonmodel) to parse JSON data into native objects and it worked pretty well. But the iOS developer landscape has changed with the shift from Objective C to Swift so I wanted to find a Swift based framework. There are a number of open source Swift JSON parsers, but the ones I tried resulted in code mountains just to parse some format of JSON. This felt like a fail compared to the elegant manner of Newtonsoft or GSON object models. I was surprised how hard it was to pinpoint the one Swift library that could satisfy all my parsing needs. But with [Argo](https://github.com/thoughtbot/Argo) I feel I've discovered the golden JSON parsing library for iOS Swift development.

## Getting on board with Argo

I'm a long time user of [CocoaPods](https://cocoapods.org/) for Xcode source control projects as it makes it easier to avoid jamming up a repro with binaries. However the precompiled versions on CocoaPods don't always provide the latest version available on GitHub. This is where [Carthage](https://github.com/Carthage/Carthage) comes in as you specifically request a tag version or branch on GitHub. Carthage can be quickly installed using [Homebrew](http://brew.sh/) as mentioned in the [installing Carthage docs].(https://github.com/Carthage/Carthage#installing-carthage)  

`brew install carthage`

To setup create a new text file and save it as '**Cartfile**' inside your Xcode project folder. (In this case I'm requesting a specific version of Argo and Curry for use with Swift 2)

```conf
github &quot;thoughtbot/Argo&quot; == 3.0.2
github &quot;thoughtbot/Curry&quot; == 2.2
```

Once you have installed Carthage and saved a 'Cartfile' then you need to build the frameworks.

1. In Terminal navigate to the project folder and run `carthage update` to build frameworks for all platforms. NB: For packages that can only be built for a single platform use `carthage build --platform iOS`
2. Drop built '\*.framework' folder into Xcode project
3. Add Build Phases \&gt; Run Script `carthage copy-frameworks` and add Input Files path to '\*.framework'

## JSON data modelling with Argo

Argo decodes standard property types (`String, Int, UInt, Int64, UInt64, Double, Float, Bool`) as well as arrays and optional properties. You can decode a nested object or an array of nested objects that conform to the 'Decodable' protocol. In fact you can even do inception - using the same struct within itself as shown below. One thing that might require explanation is [Argo's sugar syntax](https://github.com/thoughtbot/Argo/blob/master/Documentation/Basic-Usage.md#safely-pulling-values-from-json). The summary of the sugar syntax is this:

- **`&lt;^&gt;`** syntax pulls the first property, and `&lt;*&gt;` pulls subsequent properties.
- **`&lt;|`** syntax relates to a property.
- **`&lt;|?`** syntax relates to an optional property.
- **`&lt;||`** syntax relates to an array of 'decodable' objects.
- **`&lt;||?`** syntax relates to an array of optional 'decodable' objects

```swift
import Foundation
import Argo
import Curry

struct SomeModel {
    let id : String
    let name : String
    let total : Int
    let isHighlighted : Bool
    let optional : String?
    let children : [SomeModel]?
}

extension SomeModel: Decodable {
    static func decode(j: JSON) -&gt; Decoded&lt;SomeModel&gt; {
        return curry(SomeModel.init)
            &lt;^&gt; j &lt;| &quot;_id&quot;
            &lt;*&gt; j &lt;| &quot;name&quot;
            &lt;*&gt; j &lt;| &quot;total&quot;
            &lt;*&gt; j &lt;| &quot;highlighted&quot;
            &lt;*&gt; j &lt;|? &quot;optional&quot;
            &lt;*&gt; j &lt;||? &quot;children&quot;
    }
}
```

## What about decoding JSON values into native types like NSURL and NSDate?

It can be advantageous to parse URL and date values as native types instead of String types. To get this to work with Argo you need to make a parser which wraps NSURL and NSDate in the 'Decoded' type. But first I made a Uri helper to encode url strings as NSURL and a Date helper to convert a date string (of a known format) to NSDate.

```swift
import Foundation
class Uri {
    static func encodeURLString(urlString: String) -&gt; String {
        let characterSet = NSMutableCharacterSet()
        characterSet.formUnionWithCharacterSet(NSCharacterSet.URLPathAllowedCharacterSet())
        characterSet.formUnionWithCharacterSet(NSCharacterSet.URLQueryAllowedCharacterSet())
        return urlString.stringByAddingPercentEncodingWithAllowedCharacters( characterSet ) ?? urlString
    }
}
```

```swift
import Foundation

enum DateFormats : String {
    case Milliseconds = &quot;yyyy'-'MM'-'dd'T'HH':'mm':'ss'.'SSS'Z'&quot;
    case Seconds = &quot;yyyy'-'MM'-'dd'T'HH':'mm':'ss'Z'&quot;
}

class Date {
    static private let dateFormatter = NSDateFormatter()
    
    // converts String (using array of potential date formats) to Date
    static func StringToDate(dateString : String) -&gt; NSDate? {
        var date : NSDate? = nil
        let dateFormats : [String] = [DateFormats.Milliseconds.rawValue, DateFormats.Seconds.rawValue]
        for dateFormat in dateFormats {
            dateFormatter.dateFormat = dateFormat
            if let formatedDate = dateFormatter.dateFromString(dateString) {
                date = formatedDate
                break
            }
        }
        return date
    }
}
```

The Parser helper returns objects wrapped in Decoded type:

```swift
import Foundation
import Argo

class Parser {
    
    static func toNSURL(urlString : String) -&gt; Decoded&lt;NSURL&gt; {
        let urlEncodedString = Uri.encodeURLString(urlString)
        guard let url = NSURL(string: urlEncodedString) else {
            return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSURL&quot;))
        }
        // Return NSURL wrapped in .Success
        return pure(url)
    }
    
    static func toNSDate(dateString : String) -&gt; Decoded&lt;NSDate&gt; {
        guard let date = Date.StringToDate(dateString) else {
            return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSDate&quot;))
        }
        // Return NSDate wrapped in .Success
        return pure(date)
    }
    
    // optional (nil values are allowed)
    
    static func toOptionalNSDate(dateString : String?) -&gt; Decoded&lt;NSDate?&gt; {
        guard let str = dateString else {
            return pure(nil) // No date string
        }
        guard let date = Date.StringToDate(str) else {
            return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSDate&quot;))
        }
        // Return NSDate wrapped in .Success
        return pure(date)
    }
}
```

Example model with NSURL and NSDate using the Parser helper (note the extra brackets):

```swift
import Foundation
import Argo
import Curry

struct SomeModel {
    let id : String
    let url : NSURL
    let dateCreated : NSDate?
}

extension SomeModel: Decodable {
    static func decode(j: JSON) -&gt; Decoded&lt;SomeModel&gt; {
        return curry(SomeModel.init)
            &lt;^&gt; j &lt;| &quot;_id&quot;
            &lt;*&gt; (j &lt;| &quot;url&quot; &gt;&gt;- Parser.toNSURL)
            &lt;*&gt; (j &lt;|? &quot;date_created&quot; &gt;&gt;- Parser.toOptionalNSDate)
    }
}
```

## Three things to avoid in your JSON models for smoother sailing with Argo

1. Two dimensional arrays (arrays within an array) aren't handled out of the box. There are [multi-dimensional array workarounds](https://github.com/thoughtbot/Argo/issues/166) but it can cause compiler melt down if your model is particularly complex. Better to avoid this complexity by flattening arrays to a single array or use nested property arrays.
2. Best to limit object model to no more than 10 properties. This is because there are limits of how many things can be curried with Argo before the complier gives up. Try to use nested objects to group things together, but if that is not possible then there are techniques to deal with [complex expressions](https://github.com/thoughtbot/Argo/blob/master/Documentation/Compilation-Errors.md#complex-expressions).
3. Array of mixed objects (dynamic types). Argo can be made to [decode an array of different types](https://github.com/thoughtbot/Argo/issues/325) but it will increase complexity as you will have to use subclasses instead of structs.

## How to load JSON file within iOS app bundle in Swift

Often the first thing I like to do is to load a JSON file to configure my app. For example you might have various JSON config files for localhost, staging and production settings.

```json
{
    &quot;app_url&quot;: &quot;https://someapp.azurewebsites.net&quot;,
}
```

The data model using [Argo](https://github.com/thoughtbot/Argo) &amp; [Curry](https://github.com/thoughtbot/Curry) would look like this in Swift:

```swift
import Foundation
import Argo
import Curry

struct ConfigModel {
    let appUrl: String
}

extension ConfigModel: Decodable {
    static func decode(j: JSON) -&gt; Decoded&lt;ConfigModel&gt; {
        return curry(ConfigModel.init)
            &lt;^&gt; j &lt;| &quot;app_url&quot;
    }
}
```

To load the JSON file within the app bundle I use a file helper:

```swift
// returns json from file
static func loadJSON(file: String) -&gt; AnyObject? {
    let path : String? = NSBundle.mainBundle().pathForResource(file, ofType: &quot;json&quot;)
    guard let unwrappedPath = path else {
        return nil
    }
    let fileContents : NSData? = NSData(contentsOfFile: unwrappedPath)
    guard let data = fileContents else {
        return nil
    }
    do {
        return try NSJSONSerialization.JSONObjectWithData(data, options: NSJSONReadingOptions.AllowFragments)
    } catch let error as NSError {
        print(error.localizedDescription)
    }
    return nil
}
```

The loaded JSON can be parsed into the 'ConfigModel' using Argo's decode method.

```swift
func loadConfig(file:String) -&gt; ConfigModel? {
    let json : AnyObject? = loadJSON(file)
    if let j = json {
        return decode(j)
    }
    debugPrint(&quot;Error with \(file).json file&quot;)
    return nil
}
```

While this is fine for converting one type of object, what if you have multiple data models? You could quickly end up with a lot of repetitive code. One of the powerful things with Swift 2 is that it supports Abstract Types. Argo needs a little help to ensure the abstract type conforms to the Decodable type so there is slightly more boilerplate in this case, but it should help keep things DRY.

```swift
func loadJSONFile&lt;T: Decodable where T == T.DecodedType&gt;(file : String) -&gt; T? {
    let json : AnyObject? = loadJSON(file)
    if let j: AnyObject = json {
        return decode(j)
    }
    debugPrint(&quot;Error with \(file).json file&quot;)
    return nil
}
```

The JSON config file can be loaded in AppDelegate in the 'didFinishLaunchingWithOptions' method:

```swift
var config: ConfigModel? 
func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -&gt; Bool {
    config = loadJSONFile(&quot;config&quot;)
    return true
}
```

## Parsing JSON response from REST service

I also needed to parse various JSON results provided by via REST service API. To handle the REST request here I'll be using the [Alamofire](https://github.com/Alamofire/Alamofire) library for Swift. Alamofire can also be added to the Cartfile:

```conf
github &quot;Alamofire/Alamofire&quot; ~\&gt; 3.4
```

Below is an example snippet taken from a login POST request. When using Alamofire the JSON data is available as `response.result.value` which can be parsed with the Argo decode method.

```swift
func login(username: String, password: String) {
    let authURL : NSURL = NSURL(string: &quot;https://some_auth_endpoint&quot;)
    
    // Request body params
    let parameters : [String: AnyObject] = [
        &quot;username&quot;: username,
        &quot;password&quot;: password
    ]
    
    // Initiate async request using Alamofire 
    Alamofire.request(.POST, authURL, parameters: parameters, encoding: .JSON).responseJSON {
        response in
        
        // Return early on failure
        guard response.response?.statusCode == 200 else {
            let alert = UIAlertController.init(title: &quot;Error&quot;, message: &quot;Failed to login, please check username and password.&quot;, preferredStyle: .Alert)
            alert.addAction(UIAlertAction(title: &quot;Ok&quot;, style: .Cancel, handler: {(alertAction: UIAlertAction) in
                alert.dismissViewControllerAnimated(true, completion: nil)
            }))
            self.presentViewController(alert, animated: true, completion: nil)
            return
        }
        
        // Parse JSON result value using Argo
        guard let result = response.result.value,
              let authToken: AuthTokenModel = decode(result) else {
            debugPrint(&quot;Auth token model parse error&quot;)
            return
        }
        
        // Login was successful, do stuff here and then navigate to home screen...
    }
}
```

One thing to point out: I have used very simple parse error detection here - it either decodes or it doesn't and there is no indication of what went wrong during the decode process. With smaller data models this form of indication is perfectly adequate. But when you are working with complex data models then this type of error reporting is not granular enough to pinpoint the exact the problem if you get a parse error. Fortunately Argo provides a way to parse with failure reporting by using a Decoded type.

```swift
// Get JSON data from Alamofire response
guard let result = response.result.value else {
    print(&quot;No request result&quot;)
    return
}

// Try decoding model with failure reporting by using Argo's Decoded type
let decodeResult: Decoded&lt;SomeModel&gt; = decode(result)
switch(decodeResult) {
case .Failure:
    print(&quot;Failed to decode model: \(decodeResult.error?.description)&quot;)
    return
case .Success:
    print(&quot;Decode success&quot;)
}

// Assign decoded value to data model
guard let report : SomeModel = decodeResult.value else {
    print(&quot;Error unwrapping Report result&quot;)
    return
}
```

I found this an absolutely invaluable technique to be able to debug issues with my complex models, especially as models are pretty verbose and its always hard to spot that one string mistake.

## What's next…

What about storing loaded data for offline use? JSON documents can be stored with revisions using a [Couchbase Lite](http://developer.couchbase.com/documentation/mobile/1.3/develop/references/couchbase-lite/couchbase-lite/index.html) database. The problem here is Argo only accommodates decode, but the native objects will need encoded back into JSON for use with Couchbase. This is where [Ogra](https://github.com/edwardaux/Ogra) (Argo in reverse) comes in. The only thing is you will need to extend the data object with an encode method. If you found this post useful or if you would be interested to see some Ogra to Couch examples just fire me a tweet [@deadlyfingers](https://twitter.com/deadlyfingers).</content><author><name>David Douglas</name></author><category term="Alamofire" /><category term="Argo" /><category term="JSON" /><category term="Swift" /><summary type="html">Recently I started a new iOS Swift project and spent way more time than I would like trying to find a JSON parser that could handle the various JSON data models I was working with. In this post I will document some real code samples that should prove useful for other iOS developers looking to get off to head start with data modelling in Swift. The search for a Swift JSON parser… Handling JSON is a very common task with modern app development whether its consuming some REST Service API, loading a JSON file or document objects from database. With regards to Windows C# apps Newtonsoft JSON is the popular choice and similarly with Java for Android there is GSON. But what library to use for iOS apps? Previously I had used libraries like JSONModel to parse JSON data into native objects and it worked pretty well. But the iOS developer landscape has changed with the shift from Objective C to Swift so I wanted to find a Swift based framework. There are a number of open source Swift JSON parsers, but the ones I tried resulted in code mountains just to parse some format of JSON. This felt like a fail compared to the elegant manner of Newtonsoft or GSON object models. I was surprised how hard it was to pinpoint the one Swift library that could satisfy all my parsing needs. But with Argo I feel I’ve discovered the golden JSON parsing library for iOS Swift development. Getting on board with Argo I’m a long time user of CocoaPods for Xcode source control projects as it makes it easier to avoid jamming up a repro with binaries. However the precompiled versions on CocoaPods don’t always provide the latest version available on GitHub. This is where Carthage comes in as you specifically request a tag version or branch on GitHub. Carthage can be quickly installed using Homebrew as mentioned in the [installing Carthage docs].(https://github.com/Carthage/Carthage#installing-carthage) brew install carthage To setup create a new text file and save it as ‘Cartfile’ inside your Xcode project folder. (In this case I’m requesting a specific version of Argo and Curry for use with Swift 2) github &quot;thoughtbot/Argo&quot; == 3.0.2 github &quot;thoughtbot/Curry&quot; == 2.2 Once you have installed Carthage and saved a ‘Cartfile’ then you need to build the frameworks. In Terminal navigate to the project folder and run carthage update to build frameworks for all platforms. NB: For packages that can only be built for a single platform use carthage build --platform iOS Drop built ‘*.framework’ folder into Xcode project Add Build Phases &amp;gt; Run Script carthage copy-frameworks and add Input Files path to ‘*.framework’ JSON data modelling with Argo Argo decodes standard property types (String, Int, UInt, Int64, UInt64, Double, Float, Bool) as well as arrays and optional properties. You can decode a nested object or an array of nested objects that conform to the ‘Decodable’ protocol. In fact you can even do inception - using the same struct within itself as shown below. One thing that might require explanation is Argo’s sugar syntax. The summary of the sugar syntax is this: &amp;lt;^&amp;gt; syntax pulls the first property, and &amp;lt;*&amp;gt; pulls subsequent properties. &amp;lt;| syntax relates to a property. &amp;lt;|? syntax relates to an optional property. &amp;lt;|| syntax relates to an array of ‘decodable’ objects. &amp;lt;||? syntax relates to an array of optional ‘decodable’ objects import Foundation import Argo import Curry struct SomeModel { let id : String let name : String let total : Int let isHighlighted : Bool let optional : String? let children : [SomeModel]? } extension SomeModel: Decodable { static func decode(j: JSON) -&amp;gt; Decoded&amp;lt;SomeModel&amp;gt; { return curry(SomeModel.init) &amp;lt;^&amp;gt; j &amp;lt;| &quot;_id&quot; &amp;lt;*&amp;gt; j &amp;lt;| &quot;name&quot; &amp;lt;*&amp;gt; j &amp;lt;| &quot;total&quot; &amp;lt;*&amp;gt; j &amp;lt;| &quot;highlighted&quot; &amp;lt;*&amp;gt; j &amp;lt;|? &quot;optional&quot; &amp;lt;*&amp;gt; j &amp;lt;||? &quot;children&quot; } } What about decoding JSON values into native types like NSURL and NSDate? It can be advantageous to parse URL and date values as native types instead of String types. To get this to work with Argo you need to make a parser which wraps NSURL and NSDate in the ‘Decoded’ type. But first I made a Uri helper to encode url strings as NSURL and a Date helper to convert a date string (of a known format) to NSDate. import Foundation class Uri { static func encodeURLString(urlString: String) -&amp;gt; String { let characterSet = NSMutableCharacterSet() characterSet.formUnionWithCharacterSet(NSCharacterSet.URLPathAllowedCharacterSet()) characterSet.formUnionWithCharacterSet(NSCharacterSet.URLQueryAllowedCharacterSet()) return urlString.stringByAddingPercentEncodingWithAllowedCharacters( characterSet ) ?? urlString } } import Foundation enum DateFormats : String { case Milliseconds = &quot;yyyy'-'MM'-'dd'T'HH':'mm':'ss'.'SSS'Z'&quot; case Seconds = &quot;yyyy'-'MM'-'dd'T'HH':'mm':'ss'Z'&quot; } class Date { static private let dateFormatter = NSDateFormatter() // converts String (using array of potential date formats) to Date static func StringToDate(dateString : String) -&amp;gt; NSDate? { var date : NSDate? = nil let dateFormats : [String] = [DateFormats.Milliseconds.rawValue, DateFormats.Seconds.rawValue] for dateFormat in dateFormats { dateFormatter.dateFormat = dateFormat if let formatedDate = dateFormatter.dateFromString(dateString) { date = formatedDate break } } return date } } The Parser helper returns objects wrapped in Decoded type: import Foundation import Argo class Parser { static func toNSURL(urlString : String) -&amp;gt; Decoded&amp;lt;NSURL&amp;gt; { let urlEncodedString = Uri.encodeURLString(urlString) guard let url = NSURL(string: urlEncodedString) else { return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSURL&quot;)) } // Return NSURL wrapped in .Success return pure(url) } static func toNSDate(dateString : String) -&amp;gt; Decoded&amp;lt;NSDate&amp;gt; { guard let date = Date.StringToDate(dateString) else { return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSDate&quot;)) } // Return NSDate wrapped in .Success return pure(date) } // optional (nil values are allowed) static func toOptionalNSDate(dateString : String?) -&amp;gt; Decoded&amp;lt;NSDate?&amp;gt; { guard let str = dateString else { return pure(nil) // No date string } guard let date = Date.StringToDate(str) else { return Decoded.Failure(DecodeError.Custom(&quot;Failed to parse String to NSDate&quot;)) } // Return NSDate wrapped in .Success return pure(date) } } Example model with NSURL and NSDate using the Parser helper (note the extra brackets): import Foundation import Argo import Curry struct SomeModel { let id : String let url : NSURL let dateCreated : NSDate? } extension SomeModel: Decodable { static func decode(j: JSON) -&amp;gt; Decoded&amp;lt;SomeModel&amp;gt; { return curry(SomeModel.init) &amp;lt;^&amp;gt; j &amp;lt;| &quot;_id&quot; &amp;lt;*&amp;gt; (j &amp;lt;| &quot;url&quot; &amp;gt;&amp;gt;- Parser.toNSURL) &amp;lt;*&amp;gt; (j &amp;lt;|? &quot;date_created&quot; &amp;gt;&amp;gt;- Parser.toOptionalNSDate) } } Three things to avoid in your JSON models for smoother sailing with Argo Two dimensional arrays (arrays within an array) aren’t handled out of the box. There are multi-dimensional array workarounds but it can cause compiler melt down if your model is particularly complex. Better to avoid this complexity by flattening arrays to a single array or use nested property arrays. Best to limit object model to no more than 10 properties. This is because there are limits of how many things can be curried with Argo before the complier gives up. Try to use nested objects to group things together, but if that is not possible then there are techniques to deal with complex expressions. Array of mixed objects (dynamic types). Argo can be made to decode an array of different types but it will increase complexity as you will have to use subclasses instead of structs. How to load JSON file within iOS app bundle in Swift Often the first thing I like to do is to load a JSON file to configure my app. For example you might have various JSON config files for localhost, staging and production settings. { &quot;app_url&quot;: &quot;https://someapp.azurewebsites.net&quot;, } The data model using Argo &amp;amp; Curry would look like this in Swift: import Foundation import Argo import Curry struct ConfigModel { let appUrl: String } extension ConfigModel: Decodable { static func decode(j: JSON) -&amp;gt; Decoded&amp;lt;ConfigModel&amp;gt; { return curry(ConfigModel.init) &amp;lt;^&amp;gt; j &amp;lt;| &quot;app_url&quot; } } To load the JSON file within the app bundle I use a file helper: // returns json from file static func loadJSON(file: String) -&amp;gt; AnyObject? { let path : String? = NSBundle.mainBundle().pathForResource(file, ofType: &quot;json&quot;) guard let unwrappedPath = path else { return nil } let fileContents : NSData? = NSData(contentsOfFile: unwrappedPath) guard let data = fileContents else { return nil } do { return try NSJSONSerialization.JSONObjectWithData(data, options: NSJSONReadingOptions.AllowFragments) } catch let error as NSError { print(error.localizedDescription) } return nil } The loaded JSON can be parsed into the ‘ConfigModel’ using Argo’s decode method. func loadConfig(file:String) -&amp;gt; ConfigModel? { let json : AnyObject? = loadJSON(file) if let j = json { return decode(j) } debugPrint(&quot;Error with \(file).json file&quot;) return nil } While this is fine for converting one type of object, what if you have multiple data models? You could quickly end up with a lot of repetitive code. One of the powerful things with Swift 2 is that it supports Abstract Types. Argo needs a little help to ensure the abstract type conforms to the Decodable type so there is slightly more boilerplate in this case, but it should help keep things DRY. func loadJSONFile&amp;lt;T: Decodable where T == T.DecodedType&amp;gt;(file : String) -&amp;gt; T? { let json : AnyObject? = loadJSON(file) if let j: AnyObject = json { return decode(j) } debugPrint(&quot;Error with \(file).json file&quot;) return nil } The JSON config file can be loaded in AppDelegate in the ‘didFinishLaunchingWithOptions’ method: var config: ConfigModel? func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -&amp;gt; Bool { config = loadJSONFile(&quot;config&quot;) return true } Parsing JSON response from REST service I also needed to parse various JSON results provided by via REST service API. To handle the REST request here I’ll be using the Alamofire library for Swift. Alamofire can also be added to the Cartfile: github &quot;Alamofire/Alamofire&quot; ~\&amp;gt; 3.4 Below is an example snippet taken from a login POST request. When using Alamofire the JSON data is available as response.result.value which can be parsed with the Argo decode method. func login(username: String, password: String) { let authURL : NSURL = NSURL(string: &quot;https://some_auth_endpoint&quot;) // Request body params let parameters : [String: AnyObject] = [ &quot;username&quot;: username, &quot;password&quot;: password ] // Initiate async request using Alamofire Alamofire.request(.POST, authURL, parameters: parameters, encoding: .JSON).responseJSON { response in // Return early on failure guard response.response?.statusCode == 200 else { let alert = UIAlertController.init(title: &quot;Error&quot;, message: &quot;Failed to login, please check username and password.&quot;, preferredStyle: .Alert) alert.addAction(UIAlertAction(title: &quot;Ok&quot;, style: .Cancel, handler: {(alertAction: UIAlertAction) in alert.dismissViewControllerAnimated(true, completion: nil) })) self.presentViewController(alert, animated: true, completion: nil) return } // Parse JSON result value using Argo guard let result = response.result.value, let authToken: AuthTokenModel = decode(result) else { debugPrint(&quot;Auth token model parse error&quot;) return } // Login was successful, do stuff here and then navigate to home screen... } } One thing to point out: I have used very simple parse error detection here - it either decodes or it doesn’t and there is no indication of what went wrong during the decode process. With smaller data models this form of indication is perfectly adequate. But when you are working with complex data models then this type of error reporting is not granular enough to pinpoint the exact the problem if you get a parse error. Fortunately Argo provides a way to parse with failure reporting by using a Decoded type. // Get JSON data from Alamofire response guard let result = response.result.value else { print(&quot;No request result&quot;) return } // Try decoding model with failure reporting by using Argo's Decoded type let decodeResult: Decoded&amp;lt;SomeModel&amp;gt; = decode(result) switch(decodeResult) { case .Failure: print(&quot;Failed to decode model: \(decodeResult.error?.description)&quot;) return case .Success: print(&quot;Decode success&quot;) } // Assign decoded value to data model guard let report : SomeModel = decodeResult.value else { print(&quot;Error unwrapping Report result&quot;) return } I found this an absolutely invaluable technique to be able to debug issues with my complex models, especially as models are pretty verbose and its always hard to spot that one string mistake. What’s next… What about storing loaded data for offline use? JSON documents can be stored with revisions using a Couchbase Lite database. The problem here is Argo only accommodates decode, but the native objects will need encoded back into JSON for use with Couchbase. This is where Ogra (Argo in reverse) comes in. The only thing is you will need to extend the data object with an encode method. If you found this post useful or if you would be interested to see some Ogra to Couch examples just fire me a tweet @deadlyfingers.</summary></entry><entry><title type="html">Creating content with Web Components</title><link href="/code/creating-content-with-web-components" rel="alternate" type="text/html" title="Creating content with Web Components" /><published>2016-08-22T17:19:59+01:00</published><updated>2016-08-22T17:19:59+01:00</updated><id>/code/creating-content-with-web-components</id><content type="html" xml:base="/code/creating-content-with-web-components">Many web projects rely on a &lt;abbr title=&quot;Content Management System&quot;&gt;CMS&lt;/abbr&gt; of some description. The system itself is not important, but rather the content it helps to create. The primary function of a CMS is to enable the creation of content - it should empower content creation. If a new project requires a CMS the question that would tend to spring into a developer's mind is - can I use an existing CMS already out there, or do I need to build a CMS from scratch for this project? But before that can be answered, perhaps some simple questions need to be asked first.

## Asking the simple questions…

Content Management Systems are designed to make it easier to create and publish content. With so many open source systems available there's a good chance you can find something to do the job you need. Often in the case where additional functionality is required most systems can be extended with some sort of plugin to add that 'must have' feature. So why would you ever need to build your own CMS from scratch? This decision should not hang solely upon application's technical requirements, but rather it depends on who will be using it - we need to ask ourselves who will be the one creating the content? Sounds like a simple question, perhaps even an obvious question but it merits deep thought and careful design decisions. If it is a non-technical audience then displaying a bunch of features that the user doesn't need is distracting, in the worst case intimidating, ultimately leading to a poor user experience. What if you could design something from scratch so it could be tailored exactly to fit the user's requirements? Imagine if the UI only contained the functions needed without extraneous menu options or clutter and was designed to maximise ease of use and content creation.

## Starting from scratch

Recently I was working on the 'Badge Builder' project which required a CMS to author quiz content. But rather than manipulate some existing CMS or plugin that might roughly fit the use case we wondered if we could design and build our own bespoke CMS components during a one week hack. At the very outset of the project we wanted to build a system that would be easy to use and quick to create content regardless of the technical abilities of the user.

 ![Badge Builder]({{ site.baseurl }}/assets/images/BadgeBuilder.jpg)

The main problem with building all the CMS components from scratch would be the time required - with only three weeks. However there are a number of things that I feel made the most of the development time we had.

### **Web Components**

By leveraging [Web Components](http://webcomponents.org/) we could make our own custom HTML elements for each quiz and content element. Common behaviours could also be shared across elements.

### **Polymer**

During our one week hack the [Polymer Starter Kit](https://developers.google.com/web/tools/polymer-starter-kit/?hl=en) was a good kick start and saved time by setting up a stack of things like node and bower dependencies. [Polymer](https://www.polymer-project.org) provides a nice UI kit for web apps which can be separately imported for use. The PSK boilerplate is now available through [Polymer-cli](https://www.polymer-project.org/1.0/docs/tools/polymer-cli).

### **SASS and Foundation grid**

Because nobody likes working with thousands of lines of CSS, SASS can reduce physical line count and can be easily split into separate files which makes it easier to manage in source controlled projects. Also SASS makes it easy to import [Foundation Grid](http://foundation.zurb.com/grid.html) for responsive design.

### **Live reload of server and client**

A combination of [Nodemon](https://github.com/remy/nodemon) and [BrowserSync](https://www.browsersync.io/) allowed us to see live updates of all changes made on server and client side. This combo is essential to fine tune the interface and user experience and is my personal 'must have' for designing and developing a web app project.

### **Document database**

Saving content as a JSON object allowed greater freedom developing components on client side.

## Polymer Web Components

Developing Web Components for each quiz element and content element felt very intuitive. A quiz could be built using a combination of a number of individual quiz and content components.

### Quiz components:

- **Single choice**
  - Select the correct answer from a number of options

- **Multiple choice**
  - Select one or more answers that apply from a number of options

- **Ordered list**
  - Move options into their correct order using drag and drop

- **Groups**
  - Move options into their correct groups using drag and drop

- **Keywords**
  - Type keywords to answer requirements

- **Comments**
  - Type a number of words to answer

### Content components:

- **HTML**
  - HTML formatted content

- **Embedded media**
  - Embedded video player using iframe

- **Link**
  - External url

- **Section**
  - Split quiz into sections

## Reusable elements

To create [reusable Web Components](https://www.polymer-project.org/1.0/docs/tools/reusable-elements) you can use the Polymer [Seed Element](https://github.com/PolymerElements/seed-element) which sets up a test, demo and documentation page. But rather than have the overhead of managing and publishing multiple custom elements during development, it was faster to have the custom elements bundled with the project - the idea being once we had finished the project we could extract and publish them as separate elements. (One 'gotya' to be aware of is that custom element names need to be hyphenated.)

All the Web Components for the Badge Builder needed to operate on two different views - the editor (CMS) screen and the interactive viewer (quiz) screen.

### Badge Builder Editor (CMS)

![BadgeBuilder-MicroBit]({{ site.baseurl }}/assets/images/BadgeBuilder-MicroBit.jpg)

### Badge Builder Viewer (quiz)

![BadgeBuilder-MicroBit-Quiz]({{ site.baseurl }}/assets/images/BadgeBuilder-MicroBit-Quiz.jpg)

For the editor we wanted the quiz elements to be pretty &lt;abbr title=&quot;What You See Is What You Get&quot;&gt;WYSIWYG&lt;/abbr&gt; so for the most part the same element was used for the editor and viewer. The Polymer [`dom-if`](https://www.polymer-project.org/1.0/docs/api/dom-if) template was a good way to render the parts unique to each view in this case.

## Displaying dynamic content using Web Components

To render the dynamic components to the page an empty placeholder was used.

```html
&lt;div id=&quot;components&quot;&gt;&lt;/div&gt;
```

The quiz content was loaded with Polymer's [`iron-ajax`](https://elements.polymer-project.org/elements/iron-ajax) element and the array of content was parsed in the response handler using a switch statement to check against specific element types.

```js
for (var i=0; i&lt;elements.length; i++) {
  var element = elements[i];
  switch(element.elementType) {
    case &quot;content-html&quot;:
      this.addHTML(element.text);
    break;
    case &quot;content-video&quot;:
      this.addVideo(element.embededURI);
      break;
    case &quot;content-button&quot;:
      this.addButton(element.buttonURL, element.buttonText);
      break;
    case &quot;content-section&quot;:
      this.addContentSection(element.title);
      break;
    case &quot;quiz-short-input&quot;:
      this.addQuizShortInput(element._id, element.question, element.answerKeywords, element.answer, element.hintText, element.showHint);
      break;
    case &quot;quiz-long-input&quot;:
      this.addQuizLongInput(element._id, element.question, element.answer, element.wordLimit, element.hintText, element.showHint);
      break;
    case &quot;quiz-list-groups&quot;:
      this.addQuizListGroups(element._id, element.question, element.answer, element.hintText, element.showHint);
      break;
    default:
      this.addQuizElementType(element.elementType, element._id, element.question, element.options, element.answer, element.hintText, element.showHint);
      break;
  }
}
```

&lt;p&gt;Most elements are unique and are handled separately, apart from the default case which for elements that share exactly the same object properties. In this case the element type is passed to the function to create the element and set the properties by using the `document.createElement` method. (The other option is to define &lt;a href=&quot;https://www.polymer-project.org/1.0/docs/devguide/registering-elements&quot; target=&quot;_blank&quot;&gt; custom constructor&lt;/a&gt; but it's not necessary.)&lt;/p&gt;

```js
addQuizElementType: function(elementType, id, question, options, answer, hintText, showHint) {
  var el = document.createElement(elementType); // string should be a hyphenated web component
  el._id = id;
  el.question = question;
  el.options = options;
  el.answer = answer;
  el.hintText = hintText;
  el.showHint = showHint;
  this.addElement(el); // add element to the DOM
}
```

&lt;p&gt;Once the element has been created and properties set it still needs added to the DOM. This is handled with `appendChild(element)` Javascript method. Notice that we can use Polymer's &amp;#8216;`$`&amp;#8217; selector to append children to our &lt;em&gt;div&lt;/em&gt; tag with `id=&quot;components&quot;`. Because the elements are added dynamically in Javascript and therefore manipulating the DOM it is necessary to wrap the selector using the &lt;a href=&quot;https://www.polymer-project.org/1.0/docs/devguide/local-dom&quot; target=&quot;_blank&quot;&gt;Polymer DOM API&lt;/a&gt;.&lt;/p&gt;

```js
addElement: function (element, isCreated) {
  element.edit = true;
  element.classList.add(&quot;draggable&quot;);
  Polymer.dom(this.$.components).appendChild(element);
}
```

&lt;p&gt;The add element method was used when loading saved content, but also when adding new elements to the page. One usability tweak is to have the page scroll down to show a newly added component. The problem with scrolling down here is that height of the new element will not be known until the DOM has updated, so we will need to add a listener to handle the `dom-change` event. Now we can scroll down to see the element we have added.&lt;/p&gt;

```js
scrollDown: function(){
  window.scrollTo(0, document.body.scrollHeight);
},

listeners: {
  'dom-change' : &quot;scrollDown&quot;
}
```

&lt;h2&gt;Saving dynamic content using Web Components&lt;/h2&gt;
&lt;p&gt;To save the dynamic content for each element I would need to be able to get the content as JSON. A nice way to handle this for all components is to use a shared behaviour. This would hold the `_id` property assigned by the database and also assign the element's type using the built-in method `this.localName`.&lt;/p&gt;

```html
&lt;link rel=&quot;import&quot; href=&quot;../../../bower_components/polymer/polymer.html&quot;&gt;
&lt;script&gt;
  window.QuizBehaviors = window.QuizBehaviors || {}; // Behavior namespace
  /** @polymerBehavior QuizBehaviors.DataModelBehavior */
  QuizBehaviors.DataModelBehaviorImpl = {

    properties: {
      /**
       * The `id` of the element for database
       * @type {string}
       */
      _id: {
        type: String,
        value: function() {
          return &quot;&quot;;
        }
      },

    },

    /**
     * Returns the JSON data model for saving. 
     * NB: To capture more properties the `getData` method can be overridden by the custom element.
     */
     getData: function() {
       return {
         _id : this._id,
         elementType : this.localName
       };
     }

  };

  QuizBehaviors.DataModelBehavior = [
    QuizBehaviors.DataModelBehaviorImpl
  ];
&lt;/script&gt;
```

&lt;p&gt;Finally, when changes need to be saved it's just a case of returning a list of all our custom elements and grabbing the data as JSON using the element's `getData` behaviour. This data array can then be posted using Polymer's &lt;a href=&quot;https://elements.polymer-project.org/elements/iron-ajax&quot; target=&quot;_blank&quot;&gt;`iron-ajax`&lt;/a&gt; element for saving to the database.&lt;/p&gt;

```js
getElements: function(){
  return Polymer.dom(this.$.components).querySelectorAll('.draggable');
},

getElementsData: function(){
  var elementsData = [];
  var elements = this.getElements();
  var i = elements.length;
  while(i--){
    var el = elements[i];
    var data = el.getData();
    elementsData.unshift(data);
  }
  return elementsData;
}
```

You can find the [Badge Builder project](https://github.com/ideaaward/badge-builder) on github and if you want to know more you can read the full code story on the [Microsoft Developer blog](https://www.microsoft.com/developerblog/2017/05/31/digital-badge-building-reusable-web-components/).</content><author><name>David Douglas</name></author><category term="Javascript" /><category term="Polymer" /><category term="Web Components" /><summary type="html">Many web projects rely on a CMS of some description. The system itself is not important, but rather the content it helps to create. The primary function of a CMS is to enable the creation of content - it should empower content creation. If a new project requires a CMS the question that would tend to spring into a developer’s mind is - can I use an existing CMS already out there, or do I need to build a CMS from scratch for this project? But before that can be answered, perhaps some simple questions need to be asked first. Asking the simple questions… Content Management Systems are designed to make it easier to create and publish content. With so many open source systems available there’s a good chance you can find something to do the job you need. Often in the case where additional functionality is required most systems can be extended with some sort of plugin to add that ‘must have’ feature. So why would you ever need to build your own CMS from scratch? This decision should not hang solely upon application’s technical requirements, but rather it depends on who will be using it - we need to ask ourselves who will be the one creating the content? Sounds like a simple question, perhaps even an obvious question but it merits deep thought and careful design decisions. If it is a non-technical audience then displaying a bunch of features that the user doesn’t need is distracting, in the worst case intimidating, ultimately leading to a poor user experience. What if you could design something from scratch so it could be tailored exactly to fit the user’s requirements? Imagine if the UI only contained the functions needed without extraneous menu options or clutter and was designed to maximise ease of use and content creation. Starting from scratch Recently I was working on the ‘Badge Builder’ project which required a CMS to author quiz content. But rather than manipulate some existing CMS or plugin that might roughly fit the use case we wondered if we could design and build our own bespoke CMS components during a one week hack. At the very outset of the project we wanted to build a system that would be easy to use and quick to create content regardless of the technical abilities of the user. The main problem with building all the CMS components from scratch would be the time required - with only three weeks. However there are a number of things that I feel made the most of the development time we had. Web Components By leveraging Web Components we could make our own custom HTML elements for each quiz and content element. Common behaviours could also be shared across elements. Polymer During our one week hack the Polymer Starter Kit was a good kick start and saved time by setting up a stack of things like node and bower dependencies. Polymer provides a nice UI kit for web apps which can be separately imported for use. The PSK boilerplate is now available through Polymer-cli. SASS and Foundation grid Because nobody likes working with thousands of lines of CSS, SASS can reduce physical line count and can be easily split into separate files which makes it easier to manage in source controlled projects. Also SASS makes it easy to import Foundation Grid for responsive design. Live reload of server and client A combination of Nodemon and BrowserSync allowed us to see live updates of all changes made on server and client side. This combo is essential to fine tune the interface and user experience and is my personal ‘must have’ for designing and developing a web app project. Document database Saving content as a JSON object allowed greater freedom developing components on client side. Polymer Web Components Developing Web Components for each quiz element and content element felt very intuitive. A quiz could be built using a combination of a number of individual quiz and content components. Quiz components: Single choice Select the correct answer from a number of options Multiple choice Select one or more answers that apply from a number of options Ordered list Move options into their correct order using drag and drop Groups Move options into their correct groups using drag and drop Keywords Type keywords to answer requirements Comments Type a number of words to answer Content components: HTML HTML formatted content Embedded media Embedded video player using iframe Link External url Section Split quiz into sections Reusable elements To create reusable Web Components you can use the Polymer Seed Element which sets up a test, demo and documentation page. But rather than have the overhead of managing and publishing multiple custom elements during development, it was faster to have the custom elements bundled with the project - the idea being once we had finished the project we could extract and publish them as separate elements. (One ‘gotya’ to be aware of is that custom element names need to be hyphenated.) All the Web Components for the Badge Builder needed to operate on two different views - the editor (CMS) screen and the interactive viewer (quiz) screen. Badge Builder Editor (CMS) Badge Builder Viewer (quiz) For the editor we wanted the quiz elements to be pretty WYSIWYG so for the most part the same element was used for the editor and viewer. The Polymer dom-if template was a good way to render the parts unique to each view in this case. Displaying dynamic content using Web Components To render the dynamic components to the page an empty placeholder was used. &amp;lt;div id=&quot;components&quot;&amp;gt;&amp;lt;/div&amp;gt; The quiz content was loaded with Polymer’s iron-ajax element and the array of content was parsed in the response handler using a switch statement to check against specific element types. for (var i=0; i&amp;lt;elements.length; i++) { var element = elements[i]; switch(element.elementType) { case &quot;content-html&quot;: this.addHTML(element.text); break; case &quot;content-video&quot;: this.addVideo(element.embededURI); break; case &quot;content-button&quot;: this.addButton(element.buttonURL, element.buttonText); break; case &quot;content-section&quot;: this.addContentSection(element.title); break; case &quot;quiz-short-input&quot;: this.addQuizShortInput(element._id, element.question, element.answerKeywords, element.answer, element.hintText, element.showHint); break; case &quot;quiz-long-input&quot;: this.addQuizLongInput(element._id, element.question, element.answer, element.wordLimit, element.hintText, element.showHint); break; case &quot;quiz-list-groups&quot;: this.addQuizListGroups(element._id, element.question, element.answer, element.hintText, element.showHint); break; default: this.addQuizElementType(element.elementType, element._id, element.question, element.options, element.answer, element.hintText, element.showHint); break; } } Most elements are unique and are handled separately, apart from the default case which for elements that share exactly the same object properties. In this case the element type is passed to the function to create the element and set the properties by using the `document.createElement` method. (The other option is to define custom constructor but it's not necessary.) addQuizElementType: function(elementType, id, question, options, answer, hintText, showHint) { var el = document.createElement(elementType); // string should be a hyphenated web component el._id = id; el.question = question; el.options = options; el.answer = answer; el.hintText = hintText; el.showHint = showHint; this.addElement(el); // add element to the DOM } Once the element has been created and properties set it still needs added to the DOM. This is handled with `appendChild(element)` Javascript method. Notice that we can use Polymer's &amp;#8216;`$`&amp;#8217; selector to append children to our div tag with `id=&quot;components&quot;`. Because the elements are added dynamically in Javascript and therefore manipulating the DOM it is necessary to wrap the selector using the Polymer DOM API. addElement: function (element, isCreated) { element.edit = true; element.classList.add(&quot;draggable&quot;); Polymer.dom(this.$.components).appendChild(element); } The add element method was used when loading saved content, but also when adding new elements to the page. One usability tweak is to have the page scroll down to show a newly added component. The problem with scrolling down here is that height of the new element will not be known until the DOM has updated, so we will need to add a listener to handle the `dom-change` event. Now we can scroll down to see the element we have added. scrollDown: function(){ window.scrollTo(0, document.body.scrollHeight); }, listeners: { 'dom-change' : &quot;scrollDown&quot; } Saving dynamic content using Web Components To save the dynamic content for each element I would need to be able to get the content as JSON. A nice way to handle this for all components is to use a shared behaviour. This would hold the `_id` property assigned by the database and also assign the element's type using the built-in method `this.localName`. &amp;lt;link rel=&quot;import&quot; href=&quot;../../../bower_components/polymer/polymer.html&quot;&amp;gt; &amp;lt;script&amp;gt; window.QuizBehaviors = window.QuizBehaviors || {}; // Behavior namespace /** @polymerBehavior QuizBehaviors.DataModelBehavior */ QuizBehaviors.DataModelBehaviorImpl = { properties: { /** * The `id` of the element for database * @type {string} */ _id: { type: String, value: function() { return &quot;&quot;; } }, }, /** * Returns the JSON data model for saving. * NB: To capture more properties the `getData` method can be overridden by the custom element. */ getData: function() { return { _id : this._id, elementType : this.localName }; } }; QuizBehaviors.DataModelBehavior = [ QuizBehaviors.DataModelBehaviorImpl ]; &amp;lt;/script&amp;gt; Finally, when changes need to be saved it's just a case of returning a list of all our custom elements and grabbing the data as JSON using the element's `getData` behaviour. This data array can then be posted using Polymer's `iron-ajax` element for saving to the database. getElements: function(){ return Polymer.dom(this.$.components).querySelectorAll('.draggable'); }, getElementsData: function(){ var elementsData = []; var elements = this.getElements(); var i = elements.length; while(i--){ var el = elements[i]; var data = el.getData(); elementsData.unshift(data); } return elementsData; } You can find the Badge Builder project on github and if you want to know more you can read the full code story on the Microsoft Developer blog.</summary></entry><entry><title type="html">Azure App Services for Unity3D</title><link href="/tutorial/azure-app-services-for-unity3d" rel="alternate" type="text/html" title="Azure App Services for Unity3D" /><published>2016-05-16T15:42:13+01:00</published><updated>2016-05-16T15:42:13+01:00</updated><id>/tutorial/azure-app-services-for-unity3d</id><content type="html" xml:base="/tutorial/azure-app-services-for-unity3d">[Azure Mobile Services will be migrated to App Services](https://azure.microsoft.com/en-us/blog/transition-of-azure-mobile-services/) on Sept 1st 2016. To prepare for this migration I've renamed and updated the open source [Mobile Service Unity3d projects to support **Azure App Service**](https://github.com/Unity3dAzure/AppServices) going forward.

&lt;div class=&quot;video&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/R8adpelztJA?ecver=2&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;

# Using Azure App Services to create highscores leaderboard for Unity

To demonstrate the Azure App Service I have created a sample [Highscores demo for Unity](https://github.com/Unity3dAzure/AppServicesDemo) to insert, update and query a user's highscores. But to run the project in Unity Editor you will need to hook it up to an Azure App Service. Using an Azure account simply [create a new App Service in the Azure portal](https://portal.azure.com/), (for this demo I am using an App Service with Javascript backend). In a couple of minutes the Azure App Service should be up and running and ready to configure.

1. Open _Settings_, search for **Easy Tables** and add a **'Highscores'** table.
 ![AppService_1-EasyTables]({{ site.baseurl }}/assets/images/AppService_1-EasyTables.png)
2. Set all table permissions to allow anonymous access to start with. 
 ![AppService_2-TablePermissions]({{ site.baseurl }}/assets/images/AppService_2-TablePermissions.png)
3. Manage schema to add _Number_ column for **'score'** and _String_ column for **'userId'**
 ![AppService_3-ManageSchema]({{ site.baseurl }}/assets/images/AppService_3-ManageSchema.png)
4. Additionally, if you want to store user data or game scores you can enable authentication using Facebook, Twitter, Microsoft account or Google account. If you want to use the Facebook login in this demo you will need to [create a Facebook app](https://developers.facebook.com/docs/apps/register#create-app). Once you've created the Facebook app add the Facebook App ID and Secret to your Azure App Service Facebook Authentication settings.
 ![AppService_Auth]({{ site.baseurl }}/assets/images/AppService_Auth.png)
  Then configure the Facebook App Basic and Advanced settings with your Azure App Service URL:
 ![FacebookAppDomains]({{ site.baseurl }}/assets/images/FacebookAppDomains.png)
 ![FacebookAppSecureCanvasURL]({{ site.baseurl }}/assets/images/FacebookAppSecureCanvasURL.png)
 ![FacebookAppAdvancedSettings]({{ site.baseurl }}/assets/images/FacebookAppAdvancedSettings.png)
  If in doubt how to configure these settings check out the [Azure App Service documentation](https://azure.microsoft.com/en-gb/documentation/articles/app-service-mobile-how-to-configure-facebook-authentication/).
5. Once authentication is setup the **'Highscores'** table script can be edited to save **'userId'** information.
 ![AppService_4-TableInsertScript]({{ site.baseurl }}/assets/images/AppService_4-TableInsertScript.png)
```js
table.insert(function (context) {
    if (context.user) {
        context.item.userId = context.user.id;
    }
    return context.execute();
});
```
6. In addition to table scripts you can also create custom APIs. In _Settings_, search for **Easy APIs** and add an example **'hello'** API.
 ![AppService_EasyAPIs]({{ site.baseurl }}/assets/images/AppService_EasyAPIs.png)
 ![AppService_EasyAPIs-hello.js]({{ site.baseurl }}/assets/images/AppService_EasyAPIs-hello.js.png)
```js
module.exports = {
    &quot;get&quot;: function (req, res, next) {
        res.send(200, { message : &quot;Hello Unity!&quot; });
    }
}
```

Once you have setup Azure App Service you can update the Unity scene with your App Service **'https'** url and hit run!</content><author><name>David Douglas</name></author><category term="Azure" /><category term="App Services" /><category term="Unity3D" /><summary type="html">Azure Mobile Services will be migrated to App Services on Sept 1st 2016. To prepare for this migration I’ve renamed and updated the open source Mobile Service Unity3d projects to support Azure App Service going forward. Using Azure App Services to create highscores leaderboard for Unity To demonstrate the Azure App Service I have created a sample Highscores demo for Unity to insert, update and query a user’s highscores. But to run the project in Unity Editor you will need to hook it up to an Azure App Service. Using an Azure account simply create a new App Service in the Azure portal, (for this demo I am using an App Service with Javascript backend). In a couple of minutes the Azure App Service should be up and running and ready to configure. Open Settings, search for Easy Tables and add a ‘Highscores’ table. Set all table permissions to allow anonymous access to start with. Manage schema to add Number column for ‘score’ and String column for ‘userId’ Additionally, if you want to store user data or game scores you can enable authentication using Facebook, Twitter, Microsoft account or Google account. If you want to use the Facebook login in this demo you will need to create a Facebook app. Once you’ve created the Facebook app add the Facebook App ID and Secret to your Azure App Service Facebook Authentication settings. Then configure the Facebook App Basic and Advanced settings with your Azure App Service URL: If in doubt how to configure these settings check out the Azure App Service documentation. Once authentication is setup the ‘Highscores’ table script can be edited to save ‘userId’ information. table.insert(function (context) { if (context.user) { context.item.userId = context.user.id; } return context.execute(); }); In addition to table scripts you can also create custom APIs. In Settings, search for Easy APIs and add an example ‘hello’ API. module.exports = { &quot;get&quot;: function (req, res, next) { res.send(200, { message : &quot;Hello Unity!&quot; }); } } Once you have setup Azure App Service you can update the Unity scene with your App Service ‘https’ url and hit run!</summary></entry></feed>